import pandas as pd
import numpy as np
import json
import os
import glob
from sklearn.ensemble import RandomForestRegressor
from pathlib import Path

# --- [ë§ˆìŠ¤í„° ìˆ˜ì‹ ë¡œì§] ---
def t_score(s, t_mean=70.0, t_std=10.0):
    if len(s) < 2 or np.std(s) == 0: return np.full_like(s, t_mean)
    return np.clip(((s - np.mean(s)) / np.std(s)) * t_std + t_mean, 0, 100)

def calc_achieve(actual, target):
    return float((actual / target) * 100) if target and target > 0 else 0.0

def calc_gap(actual, target):
    gap_amount = float(actual - target)
    gap_rate = calc_achieve(actual, target) - 100.0 if target and target > 0 else 0.0
    return gap_amount, gap_rate

def calc_gini(x):
    x = np.sort(np.asarray(x))
    if len(x) == 0 or np.sum(x) == 0: return 0.0
    n = len(x)
    return (np.sum((2 * np.arange(1, n + 1) - n - 1) * x)) / (n * np.sum(x))

# 8ëŒ€ ìœ íš¨ í–‰ë™ ê¸°ì¤€ ë¦¬ìŠ¤íŠ¸
ATOMIC_BEHAVIORS = ['PT', 'ì‹œì—°', 'í´ë¡œì§•', 'ë‹ˆì¦ˆí™˜ê¸°', 'ëŒ€ë©´', 'ì»¨íƒ', 'ì ‘ê·¼', 'í”¼ë“œë°±']
MATRIX_METRICS = ['HIR', 'RTR', 'BCR', 'PHR']

def _zero_corr_dict():
    keys = ['ì²˜ë°©ê¸ˆì•¡'] + MATRIX_METRICS
    out = {}
    for r in keys:
        out[r] = {}
        for c in keys:
            out[r][c] = 1.0 if r == c else 0.0
    return out

def _safe_spearman(df_like):
    cols = ['ì²˜ë°©ê¸ˆì•¡'] + MATRIX_METRICS
    if df_like is None or len(df_like) < 2:
        return _zero_corr_dict()
    work = df_like.copy()
    for c in cols:
        if c not in work.columns:
            work[c] = 0.0
    work = work[cols].apply(pd.to_numeric, errors='coerce').fillna(0.0)
    if work.shape[0] < 2:
        return _zero_corr_dict()
    corr = work.corr(method='spearman').fillna(0.0)
    for c in cols:
        if c not in corr.index:
            corr.loc[c] = 0.0
        if c not in corr.columns:
            corr[c] = 0.0
    corr = corr.loc[cols, cols]
    for c in cols:
        corr.loc[c, c] = 1.0
    return corr.to_dict()

def build_period_matrices(target_df):
    """ì›”/ë¶„ê¸°ë³„ ì‹¤ì‹œê°„ ìƒê´€ê´€ê³„ + 1ê°œì›” í›„í–‰(4ì£¼ ë³´ì •) ìƒê´€ê´€ê³„."""
    empty_month = [_zero_corr_dict() for _ in range(12)]
    empty_quarter = [_zero_corr_dict() for _ in range(4)]
    if target_df is None or len(target_df) == 0:
        return {
            'monthly_correlation': empty_month,
            'monthly_adj_correlation': empty_month,
            'quarterly_correlation': empty_quarter,
            'quarterly_adj_correlation': empty_quarter,
        }

    df = target_df.copy()
    for c in ['ì›”', 'ì²˜ë°©ê¸ˆì•¡'] + MATRIX_METRICS:
        if c not in df.columns:
            df[c] = 0.0
    df['ì›”'] = pd.to_numeric(df['ì›”'], errors='coerce').fillna(0).astype(int)
    df = df[df['ì›”'].between(1, 12)]
    if df.empty:
        return {
            'monthly_correlation': empty_month,
            'monthly_adj_correlation': empty_month,
            'quarterly_correlation': empty_quarter,
            'quarterly_adj_correlation': empty_quarter,
        }
    group_keys = [k for k in ['ì›”', '__k_branch', '__k_rep', '__k_prod'] if k in df.columns]
    if not group_keys:
        group_keys = ['ì›”']
    agg_ops = {'ì²˜ë°©ê¸ˆì•¡': 'sum', 'HIR': 'mean', 'RTR': 'mean', 'BCR': 'mean', 'PHR': 'mean'}
    agg = df[group_keys + ['ì²˜ë°©ê¸ˆì•¡', 'HIR', 'RTR', 'BCR', 'PHR']].groupby(group_keys, as_index=False).agg(agg_ops)

    id_keys = [k for k in ['__k_branch', '__k_rep', '__k_prod'] if k in agg.columns]
    prev = agg[['ì›”'] + id_keys + MATRIX_METRICS].copy()
    prev['ì›”'] = prev['ì›”'] + 1
    prev = prev.rename(columns={m: f'{m}_prev' for m in MATRIX_METRICS})
    lagged = agg[['ì›”'] + id_keys + ['ì²˜ë°©ê¸ˆì•¡']].merge(prev, on=['ì›”'] + id_keys, how='left')

    monthly_raw = []
    monthly_adj = []
    for m in range(1, 13):
        raw_m = agg[agg['ì›”'] == m][['ì²˜ë°©ê¸ˆì•¡'] + MATRIX_METRICS]
        monthly_raw.append(_safe_spearman(raw_m))

        adj_cols = ['ì²˜ë°©ê¸ˆì•¡'] + [f'{x}_prev' for x in MATRIX_METRICS]
        adj_m = lagged[lagged['ì›”'] == m][adj_cols].rename(
            columns={f'{x}_prev': x for x in MATRIX_METRICS}
        )
        monthly_adj.append(_safe_spearman(adj_m))

    quarterly_raw = []
    quarterly_adj = []
    for q in range(4):
        months = [q * 3 + 1, q * 3 + 2, q * 3 + 3]
        raw_q = agg[agg['ì›”'].isin(months)][['ì²˜ë°©ê¸ˆì•¡'] + MATRIX_METRICS]
        quarterly_raw.append(_safe_spearman(raw_q))

        adj_cols = ['ì²˜ë°©ê¸ˆì•¡'] + [f'{x}_prev' for x in MATRIX_METRICS]
        adj_q = lagged[lagged['ì›”'].isin(months)][adj_cols].rename(
            columns={f'{x}_prev': x for x in MATRIX_METRICS}
        )
        quarterly_adj.append(_safe_spearman(adj_q))

    return {
        'monthly_correlation': monthly_raw,
        'monthly_adj_correlation': monthly_adj,
        'quarterly_correlation': quarterly_raw,
        'quarterly_adj_correlation': quarterly_adj,
    }

def run_full_analysis(target_df):
    if len(target_df) < 5: return None
    try:
        # X: 8 Atomic Behaviors, Y: ì²˜ë°©ê¸ˆì•¡
        # ë§Œì•½ ë°ì´í„°í”„ë ˆì„ì— í•´ë‹¹ 8ëŒ€ í–‰ë™ ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ 0ìœ¼ë¡œ ì±„ì›€
        for b in ATOMIC_BEHAVIORS:
            if b not in target_df.columns:
                target_df[b] = 0.0

        X = target_df[ATOMIC_BEHAVIORS]
        y = target_df['ì²˜ë°©ê¸ˆì•¡']
        
        # ê°’ì´ ì „ë¶€ 0ì´ë©´ ë¶„ì„ í¬ê¸°
        if X.sum().sum() == 0 or y.sum() == 0:
            return None

        rf = RandomForestRegressor(n_estimators=50, random_state=42).fit(X, y)
        importance = dict(zip(X.columns, rf.feature_importances_))
        
        # CCF ë° ìƒê´€ê´€ê³„ëŠ” ê¸°ì¡´ ì§€í‘œ(HIR, RTR, BCR, PHR) ìœ ì§€
        metrics_cols = ['HIR', 'RTR', 'BCR', 'PHR']
        for m in metrics_cols:
            if m not in target_df.columns: target_df[m] = 70.0

        ccf = [float(np.nan_to_num(y.corr(target_df['HIR'].shift(i)))) for i in range(5)]
        corr_raw = target_df[['ì²˜ë°©ê¸ˆì•¡'] + metrics_cols].corr(method='spearman').fillna(0).to_dict()
        adj_corr = target_df[['ì²˜ë°©ê¸ˆì•¡'] + metrics_cols].corr(method='spearman').fillna(0).to_dict()
        
        period_mats = build_period_matrices(target_df)
        return {
            'importance': importance,
            'ccf': ccf,
            'correlation': corr_raw,
            'adj_correlation': adj_corr,
            **period_mats
        }
    except Exception as e: 
        print(f"[WARN] run_full_analysis error: {e}")
        return None

def estimate_atomic_importance(df_slice):
    """Fallback atomic importance when model fit is unstable."""
    if df_slice is None or len(df_slice) == 0:
        return {b: 0.0 for b in ATOMIC_BEHAVIORS}
    work = df_slice.copy()
    for b in ATOMIC_BEHAVIORS:
        if b not in work.columns:
            work[b] = 0.0
    if 'ì²˜ë°©ê¸ˆì•¡' not in work.columns:
        work['ì²˜ë°©ê¸ˆì•¡'] = 0.0

    y = pd.to_numeric(work['ì²˜ë°©ê¸ˆì•¡'], errors='coerce').fillna(0.0)
    scores = {}
    for b in ATOMIC_BEHAVIORS:
        x = pd.to_numeric(work[b], errors='coerce').fillna(0.0)
        mean_x = float(np.abs(x).mean())
        if len(work) >= 2 and y.nunique() > 1 and x.nunique() > 1:
            corr = float(np.abs(x.corr(y, method='spearman')))
            if not np.isfinite(corr):
                corr = 0.0
        else:
            corr = 0.0
        scores[b] = max(0.0, corr * (mean_x + 1e-6))

    s = float(sum(scores.values()))
    if s <= 0:
        vols = {b: float(np.abs(pd.to_numeric(work[b], errors='coerce').fillna(0.0)).mean()) for b in ATOMIC_BEHAVIORS}
        v = float(sum(vols.values()))
        if v <= 0:
            return {b: 0.0 for b in ATOMIC_BEHAVIORS}
        return {b: float(vols[b] / v) for b in ATOMIC_BEHAVIORS}
    return {b: float(scores[b] / s) for b in ATOMIC_BEHAVIORS}

def summarize_activity_counts(df_slice, fallback_importance=None):
    """Aggregate 8-behavior activity volumes for detail rendering.
    When source activity is single-label sparse, distribute by fallback importance.
    """
    if df_slice is None or len(df_slice) == 0:
        return {b: 0.0 for b in ATOMIC_BEHAVIORS}
    out = {}
    for b in ATOMIC_BEHAVIORS:
        if b in df_slice.columns:
            out[b] = float(pd.to_numeric(df_slice[b], errors='coerce').fillna(0.0).sum())
        else:
            out[b] = 0.0
    total = float(sum(out.values()))
    if total <= 0:
        return out

    nonzero_behaviors = [b for b, v in out.items() if float(v) > 0]
    if len(nonzero_behaviors) <= 1:
        # Single-label collapse guard: blend observed dominant label with model importance prior
        prior = {}
        for b in ATOMIC_BEHAVIORS:
            v = 0.0
            if isinstance(fallback_importance, dict):
                try:
                    v = float(fallback_importance.get(b, 0.0))
                except Exception:
                    v = 0.0
            if not np.isfinite(v):
                v = 0.0
            prior[b] = max(0.0, v)
        s_prior = float(sum(prior.values()))
        if s_prior <= 0:
            prior = {b: 1.0 / len(ATOMIC_BEHAVIORS) for b in ATOMIC_BEHAVIORS}
        else:
            prior = {b: (prior[b] / s_prior) for b in ATOMIC_BEHAVIORS}

        dominant = nonzero_behaviors[0] if nonzero_behaviors else ATOMIC_BEHAVIORS[0]
        onehot = {b: (1.0 if b == dominant else 0.0) for b in ATOMIC_BEHAVIORS}
        alpha = 0.35  # keep observed activity signal, avoid all-or-nothing zeros
        mixed = {b: (alpha * onehot[b]) + ((1.0 - alpha) * prior[b]) for b in ATOMIC_BEHAVIORS}
        out = {b: float(total * mixed[b]) for b in ATOMIC_BEHAVIORS}
    return out

# --- [ìœ í‹¸ë¦¬í‹°: í•„ë“œ ë§¤í•‘ ì—”ì§„] ---
def load_mapping_config():
    import json
    config_path = 'config/mapping.json'
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return { # ê¸°ë³¸ ë§¤í•‘ ë°±ì—…
        "ì§€ì ": ["ì§€ì ", "ì§€ì ëª…", "Branch"],
        "ì„±ëª…": ["ì„±ëª…", "ë‹´ë‹¹ìëª…", "ë‹´ë‹¹ì", "Rep"],
        "í’ˆëª©": ["í’ˆëª©", "í’ˆëª©ëª…", "ì œí’ˆ", "Product"],
        "ì²˜ë°©ê¸ˆì•¡": ["ì²˜ë°©ê¸ˆì•¡", "ì‹¤ì ê¸ˆì•¡", "ì‹¤ì ", "Sales"],
        "ëª©í‘œê¸ˆì•¡": ["ëª©í‘œê¸ˆì•¡", "ëª©í‘œ", "Target"],
        "ì›”": ["ì›”", "ëª©í‘œì›”", "ê¸°ì¤€ì›”", "Month"]
    }

def auto_map_columns(df, mapping_dict):
    rename_plan = {}
    mapped_from = set()
    
    def process_mapping(m_dict):
        for standard_col, aliases in m_dict.items():
            if isinstance(aliases, list):
                for alias in aliases:
                    if alias in df.columns and alias not in mapped_from:
                        rename_plan[alias] = standard_col
                        mapped_from.add(alias)
                        break # ì²« ë²ˆì§¸ ë°œê²¬ëœ ë§¤ì¹­ ì‚¬ìš©
            elif isinstance(aliases, dict):
                process_mapping(aliases)
                
    process_mapping(mapping_dict)
    return df.rename(columns=rename_plan)

# --- [ìœ í‹¸ë¦¬í‹°: ìœ ë‹ˆí¬ íŒŒì¼ëª… ìƒì„±] ---
def get_unique_filename(base_dir, base_name, ext):
    from datetime import datetime
    date_str = datetime.now().strftime('%y%m%d')
    base_path = os.path.join(base_dir, f"{base_name}_{date_str}")
    
    final_path = f"{base_path}.{ext}"
    if not os.path.exists(final_path):
        return final_path
    
    counter = 1
    while True:
        final_path = f"{base_path}({counter}).{ext}"
        if not os.path.exists(final_path):
            return final_path
        counter += 1

def list_files(search_paths):
    files = []
    for path in search_paths:
        files.extend(glob.glob(path))
    uniq = []
    seen = set()
    for f in sorted(files, key=os.path.getmtime, reverse=True):
        norm = os.path.normpath(f)
        if norm not in seen:
            seen.add(norm)
            uniq.append(f)
    return uniq

def read_file(path):
    if path.endswith('.xlsx'):
        return pd.read_excel(path)
    return pd.read_csv(path)

def load_many(files, label):
    frames = []
    for path in files:
        try:
            frames.append(read_file(path))
            print(f"[LOAD:{label}] {path}")
        except Exception as e:
            print(f"[WARN:{label}] load failed: {path} ({e})")
    if not frames:
        return pd.DataFrame()
    return pd.concat(frames, ignore_index=True)

def normalize_key_series(s):
    return s.astype(str).str.strip().str.lower()

def choose_best_key_pair(df_left, left_candidates, df_right, right_candidates):
    best = None
    best_overlap = -1
    for l_col in left_candidates:
        if l_col not in df_left.columns:
            continue
        l_set = set(normalize_key_series(df_left[l_col]).dropna().tolist())
        if not l_set:
            continue
        for r_col in right_candidates:
            if r_col not in df_right.columns:
                continue
            r_set = set(normalize_key_series(df_right[r_col]).dropna().tolist())
            if not r_set:
                continue
            overlap = len(l_set & r_set)
            if overlap > best_overlap:
                best_overlap = overlap
                best = (l_col, r_col, overlap)
    return best

# --- [ë©”ì¸ ë°°í¬ ì—”ì§„] ---
def build_final_reports(external_config=None):
    print("[INFO] ë¦¬í¬íŠ¸ ë¹Œë“œ ì—”ì§„ ê°€ë™...")
    
    # 1. íŒŒì¼ ìë™ íƒìƒ‰ (í‘œì¤€ ë°ì´í„° -> sales_raw í´ë” -> ë£¨íŠ¸ ìˆœì„œ)
    # ì‹¤ì  ë°ì´í„° ê²€ìƒ‰ (ìƒŒë“œë°•ìŠ¤ ë³‘í•©ë³¸ standardized_sales ìš°ì„ )
    sales_search_paths = [
        'output/processed_data/standardized_sales_*.csv',
        'output/processed_data/standardized_sales.csv',
        'data/sales/standardized_sales.csv',
        'standardized_sales.csv',
    ]
    sales_fallback_paths = [
        'data/sales/*.xlsx',
        'data/sales/*.csv',
        '*sales*.csv',
        '*sales*.xlsx'
    ]
    
    sales_files = list_files(sales_search_paths)
    use_standardized_sales = len(sales_files) > 0
    if use_standardized_sales:
        sales_files = [sales_files[0]]
    if not sales_files:
        sales_files = list_files(sales_fallback_paths)
        use_standardized_sales = False

    if not sales_files:
        print("[ERROR] ì‹¤ì  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ëª©í‘œ ë°ì´í„° ê²€ìƒ‰
    target_search_paths = [
        'data/targets/*target*.csv',
        'data/targets/*target*.xlsx',
        'data/targets/*ëª©í‘œ*.csv',
        'data/targets/*ëª©í‘œ*.xlsx',
        'data/targets/*.csv',
        'data/targets/*.xlsx',
        '*target*.csv',
        '*target*.xlsx'
    ]
    
    target_files = list_files(target_search_paths)
    if not target_files:
        print("[ERROR] ëª©í‘œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    crm_search_paths = [
        'data/crm/*.xlsx',
        'data/crm/*.csv',
        '*crm*.xlsx',
        '*crm*.csv'
    ]
    crm_files = [] if use_standardized_sales else list_files(crm_search_paths)

    print(f"[INFO] ì‹¤ì  íŒŒì¼ ìˆ˜: {len(sales_files)}")
    print(f"[INFO] KPI ëª©í‘œ íŒŒì¼ ìˆ˜: {len(target_files)}")
    print(f"[INFO] CRM íŒŒì¼ ìˆ˜: {len(crm_files)}")
    if use_standardized_sales:
        print("[INFO] standardized_sales ë³‘í•©ë³¸ì„ ê¸°ì¤€ ë°ì´í„°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    df_raw = load_many(sales_files, 'SALES')
    df_targets = load_many(target_files, 'TARGETS')
    df_crm = load_many(crm_files, 'CRM') if crm_files else pd.DataFrame()
    
    # 0. ë™ì  ë§¤í•‘ ì„¤ì • ë¡œë“œ
    mapping_config = load_mapping_config()

    # 1. ì»¬ëŸ¼ ë§¤í•‘ ë° í‘œì¤€í™”
    df_raw = auto_map_columns(df_raw, mapping_config)
    df_targets = auto_map_columns(df_targets, mapping_config)
    if not df_crm.empty:
        df_crm = auto_map_columns(df_crm, mapping_config)

    # ë°ì´í„° í—¬ìŠ¤ ì²´í¬ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    data_health = {
        'mapped_fields': {},
        'missing_fields': [],
        'integrity_score': 100
    }

    # ë§¤í•‘ ìƒíƒœ ê¸°ë¡
    for std_col in mapping_config.keys():
        if std_col in df_raw.columns: data_health['mapped_fields'][f"Sales_{std_col}"] = "OK"
        if std_col in df_targets.columns: data_health['mapped_fields'][f"Target_{std_col}"] = "OK"

    # ëˆ„ë½ëœ ì»¬ëŸ¼ ì²˜ë¦¬ ë° ë°ì´í„° ì •ì œ (Essential: ì§€ì , ì„±ëª…, í’ˆëª©, ëª©í‘œê¸ˆì•¡)
    for col in ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']:
        if col in df_raw.columns:
            df_raw[col] = df_raw[col].astype(str).str.strip()
        if col in df_targets.columns:
            df_targets[col] = df_targets[col].astype(str).str.strip()
        if not df_crm.empty and col in df_crm.columns:
            df_crm[col] = df_crm[col].astype(str).str.strip()

    if 'ì²˜ë°©ê¸ˆì•¡' in df_raw.columns:
        df_raw['ì²˜ë°©ê¸ˆì•¡'] = pd.to_numeric(df_raw['ì²˜ë°©ê¸ˆì•¡'], errors='coerce').fillna(0)
    if 'ëª©í‘œê¸ˆì•¡' in df_raw.columns:
        df_raw['ëª©í‘œê¸ˆì•¡'] = pd.to_numeric(df_raw['ëª©í‘œê¸ˆì•¡'], errors='coerce').fillna(0)
    if 'ëª©í‘œê¸ˆì•¡' in df_targets.columns:
        df_targets['ëª©í‘œê¸ˆì•¡'] = pd.to_numeric(df_targets['ëª©í‘œê¸ˆì•¡'], errors='coerce').fillna(0)

    essential_cols = ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©', 'ëª©í‘œê¸ˆì•¡']
    for col in essential_cols:
        target_df_col = col if col in df_targets.columns else None
        if target_df_col:
            df_targets[col] = df_targets[col].astype(str).str.strip() if col != 'ëª©í‘œê¸ˆì•¡' else pd.to_numeric(df_targets[col], errors='coerce').fillna(0)
        else:
            data_health['missing_fields'].append(f"Target_{col}")
            df_targets[col] = 'Unknown' if col != 'ëª©í‘œê¸ˆì•¡' else 0
            data_health['integrity_score'] -= 15

    # ì‹¤ì  ë°ì´í„° í•„ë“œ ì²´í¬ (HIR, PHR ë“±ì„ ìœ„í•œ í•„ë“œë“¤)
    for col in ['activities', 'segment', 'ë‚ ì§œ', 'ì²˜ë°©ìˆ˜ëŸ‰']:
        if col in df_raw.columns:
            data_health['mapped_fields'][col] = col
        else:
            data_health['missing_fields'].append(col)
            # ê¸°ë³¸ê°’ ì±„ìš°ê¸° (ì—°ì‚° ì˜¤ë¥˜ ë°©ì§€)
            from datetime import datetime
            if col == 'activities': df_raw[col] = 'General'
            if col == 'segment': df_raw[col] = 'Normal'
            if col == 'ë‚ ì§œ': df_raw[col] = pd.to_datetime(datetime.now().strftime('%Y-%m-%d'))
            if col == 'ì²˜ë°©ìˆ˜ëŸ‰':
                if 'ì²˜ë°©ê¸ˆì•¡' in df_raw.columns:
                    df_raw['ì²˜ë°©ìˆ˜ëŸ‰'] = (df_raw['ì²˜ë°©ê¸ˆì•¡'] / 1000).astype(int)
                else:
                    df_raw['ì²˜ë°©ìˆ˜ëŸ‰'] = 0
            data_health['integrity_score'] -= 10
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì›”(Month) íŒŒì‹± â€“ ë‹¨ì¼ í—¬í¼ë¡œ í†µí•©í•˜ì—¬ ì¤‘ë³µ ì œê±°
    # auto_map_columnsì´ 'ëª©í‘œì›”' â†’ 'ì›”'ë¡œ ì´ë¦„ë§Œ ë°”ê¾¸ë¯€ë¡œ
    # ê°’ì´ '2026-01' ë¬¸ìì—´ì¸ ê²½ìš°ê°€ ìˆìŒ. ì´ë¥¼ ë°˜ë“œì‹œ ìˆ«ìë¡œ ë³€í™˜.
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def parse_month_col(df):
        """df ë‚´ì—ì„œ ì›”(ì •ìˆ˜ 1-12)ì„ ì¶”ì¶œí•´ ë°˜í™˜í•œë‹¤."""
        # ìš°ì„ ìˆœìœ„: 'ì›”' â†’ 'ëª©í‘œì›”' â†’ 'ë‚ ì§œ' â†’ 'í™œë™ì¼ì'
        for src in ['ì›”', 'ëª©í‘œì›”', 'ë‚ ì§œ', 'í™œë™ì¼ì']:
            if src not in df.columns:
                continue
            s = df[src]
            # ì´ë¯¸ ì •ìˆ˜í˜•ì´ë©´ ë°”ë¡œ ë°˜í™˜
            if s.dtype in ['int32', 'int64']:
                return s
            # ë‚ ì§œ/ë¬¸ìì—´ íŒŒì‹± ì‹œë„ (ì˜ˆ: '2026-01', '2026-01-15' ë“±)
            parsed = pd.to_datetime(s, errors='coerce').dt.month
            if parsed.notna().sum() > len(df) * 0.5:   # ì ˆë°˜ ì´ìƒ íŒŒì‹± ì„±ê³µ ì‹œ ì±„íƒ
                return parsed
            # ìˆ«ìë§Œ ì¶”ì¶œ ì‹œë„ (ì˜ˆ: '1', '01', '2026-01' â†’ '2026' â†’ ì›” ì•„ë‹˜,ê·¸ëƒ¥ skip)
            numeric = pd.to_numeric(s, errors='coerce')
            valid = numeric[(numeric >= 1) & (numeric <= 12)]
            if len(valid) > len(df) * 0.5:
                return numeric
        return pd.Series([1] * len(df), index=df.index)

    df_raw['ì›”']     = parse_month_col(df_raw).fillna(1).astype(int)
    df_targets['ì›”'] = parse_month_col(df_targets).fillna(1).astype(int)
    if not df_crm.empty:
        df_crm['ì›”'] = parse_month_col(df_crm).fillna(1).astype(int)

    print(f"DEBUG: Sales month dist  â†’ {df_raw['ì›”'].value_counts().sort_index().to_dict()}")
    print(f"DEBUG: Target month dist â†’ {df_targets['ì›”'].value_counts().sort_index().to_dict()}")
    if not df_crm.empty:
        print(f"DEBUG: CRM month dist    â†’ {df_crm['ì›”'].value_counts().sort_index().to_dict()}")

    # ê¸°ë³¸ ì»¬ëŸ¼ í™•ì¸ (ì²˜ë°©ìˆ˜ëŸ‰ ë“±)
    if 'ì²˜ë°©ìˆ˜ëŸ‰' not in df_raw.columns:
        df_raw['ì²˜ë°©ìˆ˜ëŸ‰'] = (df_raw['ì²˜ë°©ê¸ˆì•¡'] / 1000).astype(int) if 'ì²˜ë°©ê¸ˆì•¡' in df_raw.columns else 0

    # ë§¤ì¹­ í‚¤ ìë™ ì •í•©: ì´ë¦„/ID ì¤‘ ê²¹ì¹¨ì´ í° ì»¬ëŸ¼ ì¡°í•©ì„ ì„ íƒ
    branch_pair = choose_best_key_pair(df_raw, ['ì§€ì ', 'ì§€ì ID', 'ì§€ì ëª…'], df_targets, ['ì§€ì ', 'ì§€ì ID', 'ì§€ì ëª…'])
    rep_pair = choose_best_key_pair(df_raw, ['ì„±ëª…', 'ë‹´ë‹¹ìëª…', 'ë‹´ë‹¹ìID'], df_targets, ['ì„±ëª…', 'ë‹´ë‹¹ìëª…', 'ë‹´ë‹¹ìID'])
    prod_pair = choose_best_key_pair(df_raw, ['í’ˆëª©', 'í’ˆëª©ëª…', 'í’ˆëª©ID'], df_targets, ['í’ˆëª©', 'í’ˆëª©ëª…', 'í’ˆëª©ID'])

    if not branch_pair or not rep_pair or not prod_pair:
        print("[WARN] í‚¤ ìë™ ì •í•© ì‹¤íŒ¨: ê¸°ë³¸ í‚¤(ì§€ì /ì„±ëª…/í’ˆëª©)ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.")
        df_raw['__k_branch'] = normalize_key_series(df_raw.get('ì§€ì ', ''))
        df_raw['__k_rep'] = normalize_key_series(df_raw.get('ì„±ëª…', ''))
        df_raw['__k_prod'] = normalize_key_series(df_raw.get('í’ˆëª©', ''))
        df_targets['__k_branch'] = normalize_key_series(df_targets.get('ì§€ì ', ''))
        df_targets['__k_rep'] = normalize_key_series(df_targets.get('ì„±ëª…', ''))
        df_targets['__k_prod'] = normalize_key_series(df_targets.get('í’ˆëª©', ''))
    else:
        b_l, b_r, b_ov = branch_pair
        r_l, r_r, r_ov = rep_pair
        p_l, p_r, p_ov = prod_pair
        print(f"[INFO] í‚¤ ë§¤ì¹­ ì„ íƒ: branch({b_l}<->{b_r}, {b_ov}), rep({r_l}<->{r_r}, {r_ov}), prod({p_l}<->{p_r}, {p_ov})")
        df_raw['__k_branch'] = normalize_key_series(df_raw[b_l])
        df_raw['__k_rep'] = normalize_key_series(df_raw[r_l])
        df_raw['__k_prod'] = normalize_key_series(df_raw[p_l])
        df_targets['__k_branch'] = normalize_key_series(df_targets[b_r])
        df_targets['__k_rep'] = normalize_key_series(df_targets[r_r])
        df_targets['__k_prod'] = normalize_key_series(df_targets[p_r])

    def detect_month_col(df):
        for c in ['ì›”', 'ëª©í‘œì›”', 'Month', 'month']:
            if c in df.columns:
                return c
        return None

    def normalize_target_source(df_source, dedupe_hospital=False):
        if df_source.empty or 'ëª©í‘œê¸ˆì•¡' not in df_source.columns:
            return pd.DataFrame(columns=['__k_branch', '__k_rep', '__k_prod', '__month', 'ì§€ì ', 'ì„±ëª…', 'í’ˆëª©', 'ëª©í‘œê¸ˆì•¡'])
        month_col = detect_month_col(df_source)
        src = df_source.copy()
        if month_col is None:
            src['__month'] = 1
        else:
            src['__month'] = pd.to_numeric(src[month_col], errors='coerce').fillna(1).astype(int)
        for col in ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']:
            if col not in src.columns:
                src[col] = 'Unknown'
        for k in ['__k_branch', '__k_rep', '__k_prod']:
            if k not in src.columns:
                src[k] = ''
        src['ëª©í‘œê¸ˆì•¡'] = pd.to_numeric(src['ëª©í‘œê¸ˆì•¡'], errors='coerce').fillna(0)

        # standardized_salesëŠ” ëª©í‘œê°’ì´ ê±°ë˜/ì¼ì ë‹¨ìœ„ë¡œ ë°˜ë³µë  ìˆ˜ ìˆì–´ ë³‘ì› ë‹¨ìœ„ ì¤‘ë³µì„ ë¨¼ì € ì¶•ì†Œ
        if dedupe_hospital and 'ë³‘ì›ëª…' in src.columns:
            src['ë³‘ì›ëª…'] = src['ë³‘ì›ëª…'].astype(str).str.strip()
            src = (
                src.groupby(['__k_branch', '__k_rep', '__k_prod', '__month', 'ë³‘ì›ëª…'], as_index=False)
                .agg({
                    'ì§€ì ': 'first',
                    'ì„±ëª…': 'first',
                    'í’ˆëª©': 'first',
                    'ëª©í‘œê¸ˆì•¡': 'first'
                })
            )

        src = src[['__k_branch', '__k_rep', '__k_prod', '__month', 'ì§€ì ', 'ì„±ëª…', 'í’ˆëª©', 'ëª©í‘œê¸ˆì•¡']]
        src = src[src['ëª©í‘œê¸ˆì•¡'] > 0].copy()
        return src

    has_targets_in_standardized = (
        use_standardized_sales
        and ('ëª©í‘œê¸ˆì•¡' in df_raw.columns)
        and (pd.to_numeric(df_raw['ëª©í‘œê¸ˆì•¡'], errors='coerce').fillna(0).sum() > 0)
    )
    target_sources = []
    if has_targets_in_standardized:
        target_sources.append(normalize_target_source(df_raw, dedupe_hospital=True))
    target_sources.append(normalize_target_source(df_targets))

    target_pool = pd.concat(target_sources, ignore_index=True) if target_sources else pd.DataFrame()
    if not target_pool.empty:
        # standardized_salesì— ëª©í‘œê°€ ìˆì„ ë•Œ ì´ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ëŠ” ì¡°í•©ë§Œ target íŒŒì¼ë¡œ ë³´ê°•
        if has_targets_in_standardized and len(target_sources) > 1 and not target_sources[1].empty:
            std_keys = set(
                target_sources[0][['__k_branch', '__k_rep', '__k_prod', '__month']]
                .astype(str)
                .agg('|'.join, axis=1)
                .tolist()
            )
            target_from_file = target_sources[1].copy()
            file_keys = (
                target_from_file[['__k_branch', '__k_rep', '__k_prod', '__month']]
                .astype(str).agg('|'.join, axis=1)
            )
            target_from_file = target_from_file[~file_keys.isin(std_keys)]
            target_pool = pd.concat([target_sources[0], target_from_file], ignore_index=True)

        target_pool = (
            target_pool
            .groupby(['__k_branch', '__k_rep', '__k_prod', '__month'], as_index=False)
            .agg({
                'ì§€ì ': 'first',
                'ì„±ëª…': 'first',
                'í’ˆëª©': 'first',
                'ëª©í‘œê¸ˆì•¡': 'sum'
            })
        )

    # CRM í™œë™ëª…ì„ ì‹¤ì  ë°ì´í„°(activity)ë¡œ ë§¤í•‘
    if not df_crm.empty and 'activities' in df_crm.columns:
        for col in ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']:
            if col not in df_crm.columns:
                df_crm[col] = 'Unknown'
        df_crm['activities'] = df_crm['activities'].astype(str).str.strip()
        df_crm = df_crm[df_crm['activities'].notna() & (df_crm['activities'] != '')].copy()

        weight_col = None
        for c in ['í™˜ì‚°ì½œ(Weighted)', 'ì½œìˆ˜', 'ê°€ì¤‘ì¹˜']:
            if c in df_crm.columns:
                weight_col = c
                break
        if weight_col:
            df_crm['act_weight'] = pd.to_numeric(df_crm[weight_col], errors='coerce').fillna(1.0)
        else:
            df_crm['act_weight'] = 1.0

        act_keys = ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©', 'ì›”']
        crm_activity = (
            df_crm.groupby(act_keys + ['activities'])['act_weight']
            .sum()
            .reset_index()
            .sort_values(act_keys + ['act_weight'], ascending=[True, True, True, True, False])
            .drop_duplicates(subset=act_keys)
            [act_keys + ['activities']]
        )

        if 'activities' not in df_raw.columns:
            df_raw['activities'] = np.nan
        df_raw = df_raw.merge(crm_activity, on=act_keys, how='left', suffixes=('', '_crm'))
        if 'activities_crm' in df_raw.columns:
            df_raw['activities'] = np.where(
                df_raw['activities_crm'].notna() & (df_raw['activities_crm'].astype(str).str.strip() != ''),
                df_raw['activities_crm'],
                df_raw['activities']
            )
            df_raw = df_raw.drop(columns=['activities_crm'])

        mapped_activity_count = int(df_raw['activities'].notna().sum())
        print(f"DEBUG: CRM activity mapped rows â†’ {mapped_activity_count:,}")
        data_health['mapped_fields']['activities'] = "CRM.activities"
        if 'activities' in data_health['missing_fields']:
            data_health['missing_fields'] = [x for x in data_health['missing_fields'] if x != 'activities']
            
    # ê°€ì¤‘ì¹˜ ì„¤ì • (ìŠ¬ë¼ì´ë” ê°’ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ì—‘ì…€ì—ì„œ ë¡œë“œ)
    if external_config:
        W_ACT = external_config.get('hir_weights', {})
        W_SEG = external_config.get('pi_weights', {})
        print("[INFO] ì™¸ë¶€ ì„¤ì •(Streamlit ìŠ¬ë¼ì´ë”) ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.")
        T_MEAN, T_STD = 70.0, 10.0
    else:
        # ë§ˆìŠ¤í„° ë¡œì§ íŒŒì¼ ê²½ë¡œ ìˆ˜ì •
        logic_path = 'data/logic/SFE_Master_Logic_v1.0.xlsx'
        if not os.path.exists(logic_path):
            logic_path = 'SFE_Master_Logic_v1.0.xlsx' # ë£¨íŠ¸ í™•ì¸
            
        xl = pd.ExcelFile(logic_path)
        W_ACT = dict(zip(xl.parse('Activity_Weights')['í™œë™ëª…'], xl.parse('Activity_Weights')['ê°€ì¤‘ì¹˜']))
        W_SEG = dict(zip(xl.parse('Segment_Weights')['ë³‘ì›ê·œëª¨'], xl.parse('Segment_Weights')['ë³´ì •ê³„ìˆ˜']))
        
        try:
            sys_setup = xl.parse('System_Setup')
            T_MEAN = float(sys_setup.loc[sys_setup['ì„¤ì •í•­ëª©'].str.contains('T-Score í‰ê· ', na=False), 'ì„¤ì •ê°’'].values[0])
            T_STD = float(sys_setup.loc[sys_setup['ì„¤ì •í•­ëª©'].str.contains('T-Score í¸ì°¨', na=False), 'ì„¤ì •ê°’'].values[0])
        except Exception as e:
            print(f"[WARN] T-Score ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            T_MEAN, T_STD = 70.0, 10.0

    # 2. ì§€í‘œ ì—°ì‚° ë° 8ëŒ€ í–‰ë™ Atomic íŒŒì‹±
    # W_ACT ë”•ì…”ë„ˆë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ activities ì»¬ëŸ¼ì˜ ë¬¸ì¥ì„ íŒŒì‹±í•´ì„œ ë¹ˆë„/ê°€ì¤‘ì¹˜ ì²´í¬
    w_act_map = {str(k).strip(): v for k, v in W_ACT.items()}
    df_raw['activities'] = df_raw['activities'].astype(str).str.strip()
    
    # ê° row (ë°©ë¬¸/Call) ë³„ë¡œ 8ëŒ€ í–‰ë™ ì ìˆ˜ ë§¤í•‘ (Atomic Split)
    for b in ATOMIC_BEHAVIORS:
        # activities ë‚´ì— í•´ë‹¹ ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ìˆìœ¼ë©´ 1.0 (ë˜ëŠ” í•´ë‹¹ ê°€ì¤‘ì¹˜), ì•„ë‹ˆë©´ 0.0
        df_raw[b] = df_raw['activities'].apply(lambda x: 1.0 if b in x else 0.0)
    
    # ì „ì²´ HIR ì—°ì‚°ì„ ìœ„í•´: 8ëŒ€ í–‰ë™ * ê°€ì¤‘ì¹˜ì˜ í•©
    df_raw['HIR_W'] = 0.0
    for b in ATOMIC_BEHAVIORS:
        weight = float(w_act_map.get(b, 1.0))
        df_raw['HIR_W'] += df_raw[b] * weight
        
    df_raw['SEG_W'] = df_raw['segment'].map(W_SEG).fillna(1.0)

    # RTR: ë‚ ì§œ_ts ê°ì‡  ë¡œì§ $exp(-0.035 \times t)$
    df_raw['ë‚ ì§œ_ts'] = pd.to_datetime(df_raw['ë‚ ì§œ'], errors='coerce')
    current_time = pd.Timestamp.now()
    t_days = (current_time - df_raw['ë‚ ì§œ_ts']).dt.days.clip(lower=0)
    df_raw['RTR_raw'] = np.exp(-0.035 * t_days).fillna(0)

    print(f"DEBUG: df_raw shape: {df_raw.shape}")
    print(f"DEBUG: df_raw columns: {df_raw.columns.tolist()}")

    group_cols = ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©', '__k_branch', '__k_rep', '__k_prod']
    
    # ê° ê·¸ë£¹ë³„ë¡œ Atomic 8 í–‰ë™ ì´í•© ê³„ì‚°
    atomic_agg_dict = {b: 'sum' for b in ATOMIC_BEHAVIORS}
    agg_dict = {'ì²˜ë°©ê¸ˆì•¡': 'sum', 'ì²˜ë°©ìˆ˜ëŸ‰': 'sum', 'HIR_W': 'mean', 'RTR_raw': 'mean'}
    agg_dict.update(atomic_agg_dict)
    
    actual_agg = df_raw.groupby(group_cols).agg(agg_dict).reset_index()
    print(f"DEBUG: actual_agg shape: {actual_agg.shape}")

    # BCR: ë°©ë¬¸ ê°„ê²© í‘œì¤€í¸ì°¨ $\sigma$ ìœ ë„. ì¼ê´€ì„±ì´ ë†’ìœ¼ë©´ í‘œì¤€í¸ì°¨ ë‚®ìŒ
    df_sorted = df_raw.sort_values(group_cols + ['ë‚ ì§œ_ts'])
    df_sorted['interval'] = df_sorted.groupby(group_cols)['ë‚ ì§œ_ts'].diff().dt.days
    # ì—­ìˆ˜ë¡œ í•´ì„œ ê°’ì´ í´ìˆ˜ë¡(ê·œì¹™ì ì¼ìˆ˜ë¡) ì¢‹ê²Œ êµ¬ì„± (interval stdê°€ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
    # 0 ë¶„ëª¨ ë°©ì§€ ìœ„í•´ + 1
    bcr_raw = df_sorted.groupby(group_cols)['interval'].apply(lambda x: 1.0 / (np.std(x) + 1.0) if len(x) > 1 else 0).reset_index(name='BCR_raw')

    hir_raw = df_raw.groupby(group_cols).apply(lambda x: (x['HIR_W'] * x['SEG_W']).sum() / len(x) if len(x)>0 else 0, include_groups=False).reset_index(name='HIR_raw')
    df_master = pd.merge(actual_agg, hir_raw, on=group_cols)
    df_master = pd.merge(df_master, bcr_raw, on=group_cols, how='left')
    
    # ë§ˆìŠ¤í„° ì‹œíŠ¸ì—ì„œ ê°€ì ¸ì˜¨ T_MEAN, T_STD ë¡œ ê°€ì¤‘í‰ê·  í™˜ì‚° (T-Score)
    df_master['HIR'] = t_score(df_master['HIR_raw'].values, T_MEAN, T_STD)
    df_master['RTR'] = t_score(df_master['RTR_raw'].values, T_MEAN, T_STD)
    df_master['BCR'] = t_score(df_master['BCR_raw'].values, T_MEAN, T_STD)
    df_master['PHR'] = np.full_like(df_master['HIR'].values, T_MEAN)

    # standardized_salesì— ê¸°ì¡´ ì§€í‘œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    # standardized_salesì— ê¸°ì¡´ ì§€í‘œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    for metric in ['HIR', 'RTR', 'BCR', 'PHR']:
        target_col = None
        if f"{metric}_Raw" in df_raw.columns:
            target_col = f"{metric}_Raw"
        elif metric in df_raw.columns:
            target_col = metric
            
        if target_col:
            metric_df = df_raw[group_cols + [target_col]].copy()
            metric_df[target_col] = pd.to_numeric(metric_df[target_col], errors='coerce')
            metric_agg = metric_df.groupby(group_cols)[target_col].mean().reset_index(name=f'{metric}_src')
            df_master = df_master.merge(metric_agg, on=group_cols, how='left')
            src = df_master[f'{metric}_src']
            if src.notna().sum() > 0:
                # If values are raw (e.g. 0~5), apply t_score. If already scaled (like 0-100), just use them.
                # Usually standard raw values have small stdev
                if (src.std() or 0) > 0:
                    df_master[metric] = t_score(src.fillna(src.mean()).values, T_MEAN, T_STD)
                else:
                    df_master[metric] = np.full_like(src, T_MEAN) # ê¸°ë³¸ ì ìˆ˜
            df_master = df_master.drop(columns=[f'{metric}_src'])

    # ëª©í‘œ ë§¤ì¹­ ë° ëˆ„ë½ ì²´í¬
    if target_pool.empty:
        df_targets_agg = pd.DataFrame(columns=['__k_branch', '__k_rep', '__k_prod', 'ëª©í‘œê¸ˆì•¡'])
    else:
        df_targets_agg = (
            target_pool
            .groupby(['__k_branch', '__k_rep', '__k_prod'], as_index=False)['ëª©í‘œê¸ˆì•¡']
            .sum()
        )
    df_final = pd.merge(df_master, df_targets_agg, on=['__k_branch','__k_rep','__k_prod'], how='left')
    
    # ëˆ„ë½ ë°ì´í„° ì¶”ì¶œ (ì‹¤ì ì€ ìˆìœ¼ë‚˜ ëª©í‘œê°€ ì—†ëŠ” ê²½ìš°)
    missing_targets_df = df_final[df_final['ëª©í‘œê¸ˆì•¡'].isna() | (df_final['ëª©í‘œê¸ˆì•¡'] == 0)]
    missing_log = missing_targets_df[['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']].to_dict('records')
    
    df_final = df_final.fillna(0)
    df_final['ë‹¬ì„±ë¥ '] = np.where(df_final['ëª©í‘œê¸ˆì•¡'] > 0, (df_final['ì²˜ë°©ê¸ˆì•¡'] / df_final['ëª©í‘œê¸ˆì•¡']) * 100, 0)
    df_final['ëª©í‘œê°­'] = df_final['ì²˜ë°©ê¸ˆì•¡'] - df_final['ëª©í‘œê¸ˆì•¡']
    df_final['ëª©í‘œê°­ë¥ '] = np.where(df_final['ëª©í‘œê¸ˆì•¡'] > 0, (df_final['ì²˜ë°©ê¸ˆì•¡'] / df_final['ëª©í‘œê¸ˆì•¡'] - 1.0) * 100, 0)
    
    print(f"DEBUG: df_raw shape: {df_raw.shape}")
    print(f"DEBUG: actual_agg shape: {actual_agg.shape}")
    print(f"DEBUG: df_final shape: {df_final.shape}")
    
    if df_final.empty:
        print("[CRITICAL] df_final is empty. There is no matching data between sales and targets.")

    # --- [ì½”ì¹­ ë£° ì—”ì§„] ---
    def get_coaching_message(hir, rtr, bcr, ach, th_hir=70.0, th_rtr=70.0, th_bcr=70.0, th_ach=100.0):
        # ë§ˆìŠ¤í„° ë¡œì§ ì½”ì¹­ ë£° (êµì°¨ ê²€ì¦ ë§¤íŠ¸ë¦­ìŠ¤)
        if ach >= th_ach:
            if hir >= th_hir and rtr >= th_rtr and bcr >= th_bcr:
                return "The Masterclass", "ì™„ë²½í•œ ì„ ìˆœí™˜ì„ ë§Œë“¤ì–´ë‚´ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ì˜ ë†’ì€ í™œë™ëŸ‰ê³¼ ìš°ìˆ˜í•œ ê´€ê³„ ìœ ì§€ ëŠ¥ë ¥ì„ ìœ ì§€í•˜ë©° Best Practice ì‚¬ë¡€ë¡œ ê³µìœ ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
            elif hir >= th_hir and bcr < th_bcr:
                return "The Lucky Hunter", "ëª©í‘œëŠ” ë‹¬ì„±í–ˆìœ¼ë‚˜ ëª°ì•„ì¹˜ê¸° ì˜ì—…ì´ ì˜ì‹¬ë©ë‹ˆë‹¤. ë°©ë¬¸ ê·œì¹™ì„±(BCR)ì„ ë†’ì—¬ ì¥ê¸°ì ì´ê³  ì•ˆì •ì ì¸ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            elif hir < th_hir and rtr < th_rtr:
                return "The Data Ghost", "ëª©í‘œë¥¼ ë‹¬ì„±í–ˆìœ¼ë‚˜ í•µì‹¬ í™œë™ ë°ì´í„°(HIR, RTR)ê°€ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ìš”í–‰ì— ì˜í•œ ì‹¤ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í™œë™ ë°ì´í„° ê¸°ë¡ ë° ì¼íšŒì„± ë§¤ì¶œ ì—¬ë¶€ë¥¼ ì ê²€í•˜ì„¸ìš”."
            else:
                return "The Good Performer", "ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ì¼ë¶€ í–‰ë™ ì§€í‘œì˜ ê°œì„ ì„ í†µí•´ ë”ìš± ì™„ë²½í•œ í¼í¬ë¨¼ìŠ¤ë¥¼ ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        else:
            if hir >= th_hir and rtr >= th_rtr:
                return "The Strategic Sleeper", "í˜„ì¬ ìš°ìˆ˜í•œ í™œë™ëŸ‰ê³¼ ê´€ê³„ ì§€í‘œë¥¼ ìœ ì§€í•˜ê³  ìˆì–´ ê³§ ì‹¤ì ìœ¼ë¡œ í„°ì§ˆ ì ì¬ë ¥ì´ í½ë‹ˆë‹¤. ì„±ê³¼ì— ì¡°ê¸‰í•´í•˜ì§€ ë§ê³  í˜„ì¬ì˜ ì˜¬ë°”ë¥¸ ê³¼ì •ì„ ê¾¸ì¤€íˆ ì§€ì†í•˜ì„¸ìš”."
            elif hir < th_hir and rtr < th_rtr:
                return "The Critical Zone", "ì‹¤ì  ë¯¸ë‹¬ì„±ê³¼ í•¨ê»˜ í™œë™ëŸ‰ ë° ê´€ê³„ ì§€í‘œê°€ ëª¨ë‘ ë¬´ë„ˆì§„ ì‹¬ê°í•œ ìƒíƒœì…ë‹ˆë‹¤. ì¦‰ê°ì ì¸ ë°€ì°© ì½”ì¹­ ë° íŒŒì´í”„ë¼ì¸ ì „ë©´ ì¬ì„¤ê³„ê°€ ì‹œê¸‰í•©ë‹ˆë‹¤."
            else:
                return "The Hard Worker", "ì„±ì‹¤í•˜ê²Œ í™œë™í•˜ê³  ìˆìœ¼ë‚˜ ì„±ê³¼ë¡œ ì—°ê²°ë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤. íš¨ìœ¨ì„± ê°•í™”ë¥¼ ìœ„í•´ íƒ€ê²ŸíŒ…(Segment)ì´ë‚˜ ì£¼ë ¥ í’ˆëª©(MS) ì „ëµì˜ ì „ë©´ ì¬ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤."

    # 3. JSON ë°ì´í„° íŠ¸ë¦¬ êµ¬ì¶•
    hierarchy = {
        'branches': {}, 
        'products': sorted(df_final['í’ˆëª©'].unique().tolist()), 
        'total_avg': df_final[['HIR', 'RTR', 'BCR', 'PHR']].mean().to_dict(),
        'missing_data': missing_log, # ëˆ„ë½ëœ ë ˆì½”ë“œ ì •ë³´
        'data_health': data_health   # í•„ë“œ ë§¤í•‘ í—¬ìŠ¤ ì²´í¬ ì •ë³´ ì¶”ê°€
    }
    
    month_axis = list(range(1, 13))

    # íƒ€ê²Ÿ ì›” ë°ì´í„°ëŠ” ë§¤ì¹­ í‚¤ë¡œ sales ë¼ë²¨ì— ë§¤í•‘í•œ ë’¤ ì‚¬ìš©
    target_monthly = (
        df_targets[['__k_branch', '__k_rep', '__k_prod', 'ì›”', 'ëª©í‘œê¸ˆì•¡']]
        .merge(
            df_final[['__k_branch', '__k_rep', '__k_prod', 'ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']].drop_duplicates(),
            on=['__k_branch', '__k_rep', '__k_prod'],
            how='inner'
        )
    )

    # 2.5 ëŒ€í‘œ(Rep) ë ˆë²¨ ì§€í‘œ ì •ê·œí™” (ë³€ë³„ë ¥ í™•ë³´)
    # ê°œë³„ í’ˆëª© T-scoreì˜ í‰ê· ì„ ì“°ë©´ ë³€ë³„ë ¥ì´ ì‚¬ë¼ì§€ë¯€ë¡œ(í‰ê· íšŒê·€), Rep ë ˆë²¨ì—ì„œ Raw ì ìˆ˜ë¥¼ ë‹¤ì‹œ T-scoreí™”
    # target_pool ê¸°ë°˜ìœ¼ë¡œ ì›” íƒ€ê²Ÿì„ ì¬êµ¬ì„±í•˜ì—¬ ëˆ„ë½/ë¹ˆ ë°°ì—´ì„ ë°©ì§€
    if not target_pool.empty:
        target_monthly = (
            target_pool
            .rename(columns={'__month': 'ì›”'})
            .merge(
                df_final[['__k_branch', '__k_rep', '__k_prod', 'ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']].drop_duplicates(),
                on=['__k_branch', '__k_rep', '__k_prod'],
                how='inner',
                suffixes=('', '_sales')
            )
        )
        for c in ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']:
            sales_col = f'{c}_sales'
            if sales_col in target_monthly.columns:
                target_monthly[c] = target_monthly[sales_col].fillna(target_monthly[c])
                target_monthly = target_monthly.drop(columns=[sales_col])
    elif target_monthly.empty:
        target_monthly = pd.DataFrame(columns=['__k_branch', '__k_rep', '__k_prod', 'ì›”', 'ì§€ì ', 'ì„±ëª…', 'í’ˆëª©', 'ëª©í‘œê¸ˆì•¡'])

    df_rep_raw_calc = df_final.groupby(['ì§€ì ', 'ì„±ëª…']).agg({
        'HIR_raw': 'mean',
        'RTR_raw': 'mean',
        'BCR_raw': 'mean',
        'ì²˜ë°©ê¸ˆì•¡': 'sum',
        'ëª©í‘œê¸ˆì•¡': 'sum'
    }).reset_index()
    
    df_rep_raw_calc['REP_HIR'] = t_score(df_rep_raw_calc['HIR_raw'].values, T_MEAN, T_STD)
    df_rep_raw_calc['REP_RTR'] = t_score(df_rep_raw_calc['RTR_raw'].values, T_MEAN, T_STD)
    df_rep_raw_calc['REP_BCR'] = t_score(df_rep_raw_calc['BCR_raw'].values, T_MEAN, T_STD)
    df_rep_raw_calc['REP_ACH'] = np.where(
        df_rep_raw_calc['ëª©í‘œê¸ˆì•¡'] > 0,
        (df_rep_raw_calc['ì²˜ë°©ê¸ˆì•¡'] / df_rep_raw_calc['ëª©í‘œê¸ˆì•¡']) * 100,
        0
    )
    
    # ì ˆëŒ€í‰ê°€ ê¸°ì¤€ (T_MEAN í•˜ë“œì½”ë”©)
    th_hir = float(T_MEAN)
    th_rtr = float(T_MEAN)
    th_bcr = float(T_MEAN)
    th_ach = 100.0
    
    print(f"DEBUG: Coaching Thresholds (Absolute) -> HIR:{th_hir:.1f}, RTR:{th_rtr:.1f}, BCR:{th_bcr:.1f}, ACH:{th_ach:.1f}")

    for br in df_final['ì§€ì '].unique():
        df_br = df_final[df_final['ì§€ì '] == br]
        df_br_raw = df_raw[df_raw['ì§€ì '] == br]
        hierarchy['branches'][br] = {
            'members': [],
            'avg': df_br[['HIR', 'RTR', 'BCR', 'PHR']].mean().to_dict(),
            'achieve': calc_achieve(df_br['ì²˜ë°©ê¸ˆì•¡'].sum(), df_br['ëª©í‘œê¸ˆì•¡'].sum()),
            'actual_sum': float(df_br['ì²˜ë°©ê¸ˆì•¡'].sum()),
            'target_sum': float(df_br['ëª©í‘œê¸ˆì•¡'].sum()),
            'gap_amount': float(calc_gap(df_br['ì²˜ë°©ê¸ˆì•¡'].sum(), df_br['ëª©í‘œê¸ˆì•¡'].sum())[0]),
            'gap_rate': float(calc_gap(df_br['ì²˜ë°©ê¸ˆì•¡'].sum(), df_br['ëª©í‘œê¸ˆì•¡'].sum())[1]),
            'monthly_actual': df_raw[df_raw['ì§€ì '] == br].groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
            'monthly_target': target_monthly[target_monthly['ì§€ì '] == br].groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
            'analysis': run_full_analysis(df_br_raw),
            'prod_analysis': {pd: {
                'analysis': run_full_analysis(df_br_raw[df_br_raw['í’ˆëª©']==pd]),
                'monthly_actual': df_raw[(df_raw['ì§€ì '] == br) & (df_raw['í’ˆëª©'] == pd)].groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
                'monthly_target': target_monthly[(target_monthly['ì§€ì '] == br) & (target_monthly['í’ˆëª©'] == pd)].groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
                'achieve': calc_achieve(df_br[df_br['í’ˆëª©']==pd]['ì²˜ë°©ê¸ˆì•¡'].sum(), df_br[df_br['í’ˆëª©']==pd]['ëª©í‘œê¸ˆì•¡'].sum()),
                'actual_sum': float(df_br[df_br['í’ˆëª©']==pd]['ì²˜ë°©ê¸ˆì•¡'].sum()),
                'target_sum': float(df_br[df_br['í’ˆëª©']==pd]['ëª©í‘œê¸ˆì•¡'].sum()),
                'gap_amount': float(calc_gap(df_br[df_br['í’ˆëª©']==pd]['ì²˜ë°©ê¸ˆì•¡'].sum(), df_br[df_br['í’ˆëª©']==pd]['ëª©í‘œê¸ˆì•¡'].sum())[0]),
                'gap_rate': float(calc_gap(df_br[df_br['í’ˆëª©']==pd]['ì²˜ë°©ê¸ˆì•¡'].sum(), df_br[df_br['í’ˆëª©']==pd]['ëª©í‘œê¸ˆì•¡'].sum())[1]),
                'avg': df_br[df_br['í’ˆëª©']==pd][['HIR','RTR','BCR','PHR']].mean().to_dict()
            } for pd in hierarchy['products']}
        }
        
        for rep in df_br['ì„±ëª…'].unique():
            df_rep = df_br[df_br['ì„±ëª…'] == rep]
            rep_raw = df_raw[(df_raw['ì§€ì '] == br) & (df_raw['ì„±ëª…'] == rep)]
            rep_analysis = run_full_analysis(rep_raw)
            if rep_analysis is not None:
                real_shap = {k: float(v) for k, v in rep_analysis['importance'].items()}
            else:
                real_shap = {b: np.nan for b in ATOMIC_BEHAVIORS}
            
            prod_matrix = []
            total_sales = float(rep_raw['ì²˜ë°©ê¸ˆì•¡'].sum()) if not rep_raw.empty else 0.0
            if total_sales > 0 and not rep_raw.empty:
                max_m = rep_raw['ì›”'].max()
                prev_m = max_m - 1
                for pd_name in hierarchy['products']:
                    p_data = rep_raw[rep_raw['í’ˆëª©'] == pd_name]
                    p_sales = float(p_data['ì²˜ë°©ê¸ˆì•¡'].sum())
                    ms = (p_sales / total_sales * 100) if total_sales else 0.0
                    cm_sales = float(p_data[p_data['ì›”'] == max_m]['ì²˜ë°©ê¸ˆì•¡'].sum())
                    pm_sales = float(p_data[p_data['ì›”'] == prev_m]['ì²˜ë°©ê¸ˆì•¡'].sum())
                    if pm_sales > 0:
                        growth = ((cm_sales - pm_sales) / pm_sales) * 100
                    else:
                        growth = 100.0 if cm_sales > 0 else 0.0
                    prod_matrix.append({'name': pd_name, 'ms': ms, 'growth': growth})
            else:
                prod_matrix = [{'name': pd_name, 'ms': 0.0, 'growth': 0.0} for pd_name in hierarchy['products']]
            
            # 4ë¶„ë©´ ì „ëµ ê°€ì´ë“œë¼ì¸: ê¸°ì¤€ì„  ê³„ì‚° ë° ì½”ì¹­ ì•¡ì…˜ íŠ¸ë¦¬ê±°
            ms_values = [p['ms'] for p in prod_matrix if p['ms'] > 0]
            avg_ms = float(sum(ms_values) / len(ms_values)) if ms_values else 0.0
            
            # ì½”ì¹­ ë©”ì‹œì§€ ì—°ì‚°
            rep_stats = df_rep_raw_calc[df_rep_raw_calc['ì„±ëª…'] == rep].iloc[0]
            rep_hir = float(rep_stats['REP_HIR'])
            rep_rtr = float(rep_stats['REP_RTR'])
            rep_bcr = float(rep_stats['REP_BCR'])
            rep_ach = float(rep_stats['REP_ACH'])
            
            c_name, c_action = get_coaching_message(rep_hir, rep_rtr, rep_bcr, rep_ach, th_hir, th_rtr, th_bcr, th_ach)

            # Dog(Low MS / Low Growth) ë˜ëŠ” Question Mark(Low MS / High Growth) íŒŒì•… 
            # (ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ msê°€ í‰ê·  ë¯¸ë§Œì¸ ì£¼ë ¥/ë¹„ì£¼ë ¥ í’ˆëª© ì¤‘ ì˜ë¯¸ ìˆëŠ” ë³¼ë¥¨ ì¶”ì )
            weak_products = [p['name'] for p in prod_matrix if p['ms'] > 0 and p['ms'] < (avg_ms * 0.7) and p['growth'] < 0]
            if weak_products:
                c_action += f" (ğŸš¨ ì£¼ì˜: {', '.join(weak_products)} í’ˆëª©ì´ Dog ì˜ì—­ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. í’ˆëª© ì„±ì¥ì´ ì €ì¡°í•˜ë¯€ë¡œ íƒ€ê²ŸíŒ… ì „ëµ ì¬ìˆ˜ë¦½ì´ í•„ìš”í•©ë‹ˆë‹¤.)"

            rep_prod_analysis = {}
            rep_total_sales = float(pd.to_numeric(rep_raw['ì²˜ë°©ê¸ˆì•¡'], errors='coerce').fillna(0.0).sum()) if 'ì²˜ë°©ê¸ˆì•¡' in rep_raw.columns else 0.0
            rep_total_act = {b: float(pd.to_numeric(rep_raw[b], errors='coerce').fillna(0.0).sum()) if b in rep_raw.columns else 0.0 for b in ATOMIC_BEHAVIORS}
            rep_activity_counts = summarize_activity_counts(rep_raw, real_shap)
            for pd_name in hierarchy['products']:
                df_rep_prod = df_rep[df_rep['í’ˆëª©'] == pd_name]
                df_rep_raw_prod = rep_raw[rep_raw['í’ˆëª©'] == pd_name]
                rep_prod_ana = run_full_analysis(df_rep_raw_prod)
                if rep_prod_ana is not None and (rep_prod_ana or {}).get('importance'):
                    rep_prod_shap = {k: float(v) for k, v in rep_prod_ana.get('importance', {}).items()}
                else:
                    weighted = {}
                    prod_sales = float(pd.to_numeric(df_rep_prod['ì²˜ë°©ê¸ˆì•¡'], errors='coerce').fillna(0.0).sum()) if 'ì²˜ë°©ê¸ˆì•¡' in df_rep_prod.columns else 0.0
                    sales_share = (prod_sales / rep_total_sales) if rep_total_sales > 0 else 0.0
                    for b in ATOMIC_BEHAVIORS:
                        base_imp = float((real_shap or {}).get(b, 0.0))
                        prod_act = float(pd.to_numeric(df_rep_raw_prod[b], errors='coerce').fillna(0.0).sum()) if b in df_rep_raw_prod.columns else 0.0
                        total_act = float(rep_total_act.get(b, 0.0))
                        act_share = (prod_act / total_act) if total_act > 0 else 0.0
                        mix_share = 0.5 * act_share + 0.5 * sales_share
                        weighted[b] = max(0.0, base_imp * mix_share)
                    if sum(weighted.values()) > 0:
                        rep_prod_shap = {b: float(weighted[b]) for b in ATOMIC_BEHAVIORS}
                    else:
                        rep_prod_shap = estimate_atomic_importance(df_rep_raw_prod)
                if sum(abs(float(v or 0.0)) for v in rep_prod_shap.values()) <= 0:
                    rep_prod_shap = {k: float(v) for k, v in (real_shap or {}).items()}
                rep_prod_analysis[pd_name] = {
                    'analysis': rep_prod_ana,
                    'shap': rep_prod_shap,
                    'activity_counts': summarize_activity_counts(df_rep_raw_prod, rep_prod_shap),
                    'achieve': calc_achieve(df_rep_prod['ì²˜ë°©ê¸ˆì•¡'].sum(), df_rep_prod['ëª©í‘œê¸ˆì•¡'].sum()),
                    'actual_sum': float(df_rep_prod['ì²˜ë°©ê¸ˆì•¡'].sum()),
                    'target_sum': float(df_rep_prod['ëª©í‘œê¸ˆì•¡'].sum()),
                    'gap_amount': float(calc_gap(df_rep_prod['ì²˜ë°©ê¸ˆì•¡'].sum(), df_rep_prod['ëª©í‘œê¸ˆì•¡'].sum())[0]),
                    'gap_rate': float(calc_gap(df_rep_prod['ì²˜ë°©ê¸ˆì•¡'].sum(), df_rep_prod['ëª©í‘œê¸ˆì•¡'].sum())[1]),
                    'avg': df_rep_prod[['HIR', 'RTR', 'BCR', 'PHR']].mean().to_dict(),
                    'HIR': float(df_rep_prod['HIR'].mean()) if not df_rep_prod.empty else 0.0,
                    'RTR': float(df_rep_prod['RTR'].mean()) if not df_rep_prod.empty else 0.0,
                    'BCR': float(df_rep_prod['BCR'].mean()) if not df_rep_prod.empty else 0.0,
                    'PHR': float(df_rep_prod['PHR'].mean()) if not df_rep_prod.empty else 0.0,
                    'monthly_actual': df_raw[(df_raw['ì§€ì '] == br) & (df_raw['ì„±ëª…'] == rep) & (df_raw['í’ˆëª©'] == pd_name)]
                        .groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
                    'monthly_target': target_monthly[(target_monthly['ì§€ì '] == br) & (target_monthly['ì„±ëª…'] == rep) & (target_monthly['í’ˆëª©'] == pd_name)]
                        .groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
                }

            hierarchy['branches'][br]['members'].append({
                'ì„±ëª…': rep,
                'HIR': rep_hir, 'RTR': rep_rtr, 'BCR': rep_bcr, 'PHR': float(df_rep['PHR'].mean()),
                'ì²˜ë°©ê¸ˆì•¡': float(df_rep['ì²˜ë°©ê¸ˆì•¡'].sum()), 'ëª©í‘œê¸ˆì•¡': float(df_rep['ëª©í‘œê¸ˆì•¡'].sum()),
                'achieve': calc_achieve(df_rep['ì²˜ë°©ê¸ˆì•¡'].sum(), df_rep['ëª©í‘œê¸ˆì•¡'].sum()),
                'gap_amount': float(calc_gap(df_rep['ì²˜ë°©ê¸ˆì•¡'].sum(), df_rep['ëª©í‘œê¸ˆì•¡'].sum())[0]),
                'gap_rate': float(calc_gap(df_rep['ì²˜ë°©ê¸ˆì•¡'].sum(), df_rep['ëª©í‘œê¸ˆì•¡'].sum())[1]),
                'ì§€ì ìˆœìœ„': int(df_br.groupby('ì„±ëª…')['ì²˜ë°©ê¸ˆì•¡'].sum().rank(ascending=False)[rep]),
                'shap': real_shap,
                'coach_scenario': c_name,
                'coach_action': c_action,
                'efficiency': float(df_rep['ì²˜ë°©ê¸ˆì•¡'].sum() / (rep_raw['HIR_W'].sum() + 1)) if not rep_raw.empty else 0.0,
                'gini': float(calc_gini(df_rep['ì²˜ë°©ê¸ˆì•¡'])),
                'avg_ms': avg_ms,
                'prod_matrix': prod_matrix,
                'activity_counts': rep_activity_counts,
                'prod_analysis': rep_prod_analysis,
                'monthly_actual': df_raw[(df_raw['ì§€ì ']==br) & (df_raw['ì„±ëª…']==rep)].groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
                'monthly_target': target_monthly[(target_monthly['ì§€ì ']==br) & (target_monthly['ì„±ëª…']==rep)].groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist()
            })

    hierarchy['total_prod_analysis'] = { pd: {
        'analysis': run_full_analysis(df_raw[df_raw['í’ˆëª©']==pd]),
        'monthly_actual': df_raw[df_raw['í’ˆëª©']==pd].groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
        'monthly_target': target_monthly[target_monthly['í’ˆëª©']==pd].groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
        'achieve': calc_achieve(df_final[df_final['í’ˆëª©']==pd]['ì²˜ë°©ê¸ˆì•¡'].sum(), df_final[df_final['í’ˆëª©']==pd]['ëª©í‘œê¸ˆì•¡'].sum()),
        'actual_sum': float(df_final[df_final['í’ˆëª©']==pd]['ì²˜ë°©ê¸ˆì•¡'].sum()),
        'target_sum': float(df_final[df_final['í’ˆëª©']==pd]['ëª©í‘œê¸ˆì•¡'].sum()),
        'gap_amount': float(calc_gap(df_final[df_final['í’ˆëª©']==pd]['ì²˜ë°©ê¸ˆì•¡'].sum(), df_final[df_final['í’ˆëª©']==pd]['ëª©í‘œê¸ˆì•¡'].sum())[0]),
        'gap_rate': float(calc_gap(df_final[df_final['í’ˆëª©']==pd]['ì²˜ë°©ê¸ˆì•¡'].sum(), df_final[df_final['í’ˆëª©']==pd]['ëª©í‘œê¸ˆì•¡'].sum())[1]),
        'avg': df_final[df_final['í’ˆëª©']==pd][['HIR','RTR','BCR','PHR']].mean().to_dict()
    } for pd in hierarchy['products']}

    hierarchy['total'] = {
        'analysis': run_full_analysis(df_raw), 'avg': hierarchy['total_avg'],
        'monthly_actual': df_raw.groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
        'monthly_target': target_monthly.groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
        'achieve': calc_achieve(df_final['ì²˜ë°©ê¸ˆì•¡'].sum(), df_final['ëª©í‘œê¸ˆì•¡'].sum()),
        'actual_sum': float(df_final['ì²˜ë°©ê¸ˆì•¡'].sum()),
        'target_sum': float(df_final['ëª©í‘œê¸ˆì•¡'].sum()),
        'gap_amount': float(calc_gap(df_final['ì²˜ë°©ê¸ˆì•¡'].sum(), df_final['ëª©í‘œê¸ˆì•¡'].sum())[0]),
        'gap_rate': float(calc_gap(df_final['ì²˜ë°©ê¸ˆì•¡'].sum(), df_final['ëª©í‘œê¸ˆì•¡'].sum())[1])
    }

    # 4. íŒŒì¼ ìƒì„±
    template_path = 'templates/report_template.html'
    if not os.path.exists(template_path):
        template_path = 'report_template.html'
        
    with open(template_path, 'r', encoding='utf-8') as f: template = f.read()
    
    class SafeEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, (np.integer, np.int64)): return int(obj)
            if isinstance(obj, (np.floating, np.float64)): 
                return float(obj) if not (np.isnan(obj) or np.isinf(obj)) else 0.0
            return super().default(obj)

    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„± (íŒŒì¼ëª… ê·œì¹™ ì ìš©)
    output_path = get_unique_filename('output', 'Strategic_Full_Dashboard', 'html')
    total_json = json.dumps(hierarchy, cls=SafeEncoder, ensure_ascii=False)
    
    # í…œí”Œë¦¿ ë‚´ì˜ ë°ì´í„° ì£¼ì… (ë”ìš± ê°•ë ¥í•œ ë§¤í•‘)
    import re
    
    # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ 'const db = /*DATA_JSON_PLACEHOLDER*/ { ... };' íŒ¨í„´ì„ ì°¾ì•„ ì „ì²´ êµì²´
    # íŒ¨í„´: 'const db = ' ë’¤ì— ì£¼ì„ í˜¹ì€ ë°ì´í„°ê°€ ì˜¤ê³  ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ì§€ì ê¹Œì§€
    pattern = r'const\s+db\s*=\s*/\*DATA_JSON_PLACEHOLDER\*/\s*.*?;'
    replacement = f'const db = {total_json};'
    
    if re.search(pattern, template, flags=re.S):
        template = re.sub(pattern, replacement, template, count=1, flags=re.S)
        print("[INFO] í…œí”Œë¦¿ ë°ì´í„° ì£¼ì… ì™„ë£Œ (ì •ê·œí‘œí˜„ì‹ ë§¤ì¹­)")
    elif '/*DATA_JSON_PLACEHOLDER*/' in template:
        # ì •ê·œí‘œí˜„ì‹ì´ ì‹¤íŒ¨í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë‹¨ìˆœ ë¬¸ìì—´ êµì²´ ì‹œë„
        # í…œí”Œë¦¿ì˜ ì´ˆê¸° ê°ì²´ êµ¬ì¡°ì™€ ìƒê´€ì—†ì´ ì£¼ì„ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ êµì²´
        template = re.sub(r'/\*DATA_JSON_PLACEHOLDER\*/\s*.*?;', f'{total_json};', template, count=1, flags=re.S)
        print("[INFO] í…œí”Œë¦¿ ë°ì´í„° ì£¼ì… ì™„ë£Œ (ì£¼ì„ ê¸°ì¤€ ë§¤ì¹­)")
    else:
        print("[ERROR] í…œí”Œë¦¿ì—ì„œ ë°ì´í„° ì£¼ì… ì§€ì (DATA_JSON_PLACEHOLDER)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    template = template.replace('{{BRANCH_NAME}}', 'ì „ì‚¬')
    template = template.replace('{{BRANCH_FILTER_CLASS}}', 'v-block')
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(template)
    
    # ìµœì¢… ë°ì´í„° ìƒíƒœ ìš”ì•½ ì¶œë ¥
    print("[INFO] REPORT SUMMARY:")
    print(f"   - Match Count (df_final): {len(df_final)}")
    print(f"   - Branch Count: {len(hierarchy['branches'])}")
    print(f"   - Product Count: {len(hierarchy['products'])}")
    print(f"   - Missing Targets: {len(hierarchy['missing_data'])} items")
    
    # ë§Œì•½ ë°ì´í„°ê°€ ë„ˆë¬´ ì—†ìœ¼ë©´ ê²½ê³ 
    if len(hierarchy['branches']) == 0:
        print("[WARN] No branch data generated. The report will be empty.")
    
    print(f"[OK] '{output_path}' has been created.")
    return output_path

if __name__ == "__main__":
    build_final_reports()
