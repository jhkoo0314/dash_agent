import pandas as pd
import numpy as np
import json
import os
import glob
from sklearn.ensemble import RandomForestRegressor
from pathlib import Path

# --- [ë§ˆìŠ¤í„° ìˆ˜ì‹ ë¡œì§] ---
def t_score(s, t_mean=70.0, t_std=10.0):
    if len(s) < 2 or np.std(s) == 0: return np.full_like(s, t_mean)
    return np.clip(((s - np.mean(s)) / np.std(s)) * t_std + t_mean, 0, 100)

def calc_achieve(actual, target):
    return float((actual / target) * 100) if target and target > 0 else 0.0

def calc_gini(x):
    x = np.sort(np.asarray(x))
    if len(x) == 0 or np.sum(x) == 0: return 0.0
    n = len(x)
    return (np.sum((2 * np.arange(1, n + 1) - n - 1) * x)) / (n * np.sum(x))

# 8ëŒ€ ìœ íš¨ í–‰ë™ ê¸°ì¤€ ë¦¬ìŠ¤íŠ¸
ATOMIC_BEHAVIORS = ['PT', 'ì‹œì—°', 'í´ë¡œì§•', 'ë‹ˆì¦ˆí™˜ê¸°', 'ëŒ€ë©´', 'ì»¨íƒ', 'ì ‘ê·¼', 'í”¼ë“œë°±']

def run_full_analysis(target_df):
    if len(target_df) < 5: return None
    try:
        # X: 8 Atomic Behaviors, Y: ì²˜ë°©ê¸ˆì•¡
        # ë§Œì•½ ë°ì´í„°í”„ë ˆì„ì— í•´ë‹¹ 8ëŒ€ í–‰ë™ ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ 0ìœ¼ë¡œ ì±„ì›€
        for b in ATOMIC_BEHAVIORS:
            if b not in target_df.columns:
                target_df[b] = 0.0

        X = target_df[ATOMIC_BEHAVIORS]
        y = target_df['ì²˜ë°©ê¸ˆì•¡']
        
        # ê°’ì´ ì „ë¶€ 0ì´ë©´ ë¶„ì„ í¬ê¸°
        if X.sum().sum() == 0 or y.sum() == 0:
            return None

        rf = RandomForestRegressor(n_estimators=50, random_state=42).fit(X, y)
        importance = dict(zip(X.columns, rf.feature_importances_))
        
        # CCF ë° ìƒê´€ê´€ê³„ëŠ” ê¸°ì¡´ ì§€í‘œ(HIR, RTR, BCR, PHR) ìœ ì§€
        metrics_cols = ['HIR', 'RTR', 'BCR', 'PHR']
        for m in metrics_cols:
            if m not in target_df.columns: target_df[m] = 70.0

        ccf = [float(np.nan_to_num(y.corr(target_df['HIR'].shift(i)))) for i in range(5)]
        corr_raw = target_df[['ì²˜ë°©ê¸ˆì•¡'] + metrics_cols].corr(method='spearman').fillna(0).to_dict()
        adj_corr = target_df[['ì²˜ë°©ê¸ˆì•¡'] + metrics_cols].corr(method='spearman').fillna(0).to_dict()
        
        return {'importance': importance, 'ccf': ccf, 'correlation': corr_raw, 'adj_correlation': adj_corr}
    except Exception as e: 
        print(f"[WARN] run_full_analysis error: {e}")
        return None

# --- [ìœ í‹¸ë¦¬í‹°: í•„ë“œ ë§¤í•‘ ì—”ì§„] ---
def load_mapping_config():
    import json
    config_path = 'config/mapping.json'
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return { # ê¸°ë³¸ ë§¤í•‘ ë°±ì—…
        "ì§€ì ": ["ì§€ì ", "ì§€ì ëª…", "Branch"],
        "ì„±ëª…": ["ì„±ëª…", "ë‹´ë‹¹ìëª…", "ë‹´ë‹¹ì", "Rep"],
        "í’ˆëª©": ["í’ˆëª©", "í’ˆëª©ëª…", "ì œí’ˆ", "Product"],
        "ì²˜ë°©ê¸ˆì•¡": ["ì²˜ë°©ê¸ˆì•¡", "ì‹¤ì ê¸ˆì•¡", "ì‹¤ì ", "Sales"],
        "ëª©í‘œê¸ˆì•¡": ["ëª©í‘œê¸ˆì•¡", "ëª©í‘œ", "Target"],
        "ì›”": ["ì›”", "ëª©í‘œì›”", "ê¸°ì¤€ì›”", "Month"]
    }

def auto_map_columns(df, mapping_dict):
    rename_plan = {}
    mapped_from = set()
    
    def process_mapping(m_dict):
        for standard_col, aliases in m_dict.items():
            if isinstance(aliases, list):
                for alias in aliases:
                    if alias in df.columns and alias not in mapped_from:
                        rename_plan[alias] = standard_col
                        mapped_from.add(alias)
                        break # ì²« ë²ˆì§¸ ë°œê²¬ëœ ë§¤ì¹­ ì‚¬ìš©
            elif isinstance(aliases, dict):
                process_mapping(aliases)
                
    process_mapping(mapping_dict)
    return df.rename(columns=rename_plan)

# --- [ìœ í‹¸ë¦¬í‹°: ìœ ë‹ˆí¬ íŒŒì¼ëª… ìƒì„±] ---
def get_unique_filename(base_dir, base_name, ext):
    from datetime import datetime
    date_str = datetime.now().strftime('%y%m%d')
    base_path = os.path.join(base_dir, f"{base_name}_{date_str}")
    
    final_path = f"{base_path}.{ext}"
    if not os.path.exists(final_path):
        return final_path
    
    counter = 1
    while True:
        final_path = f"{base_path}({counter}).{ext}"
        if not os.path.exists(final_path):
            return final_path
        counter += 1

def list_files(search_paths):
    files = []
    for path in search_paths:
        files.extend(glob.glob(path))
    uniq = []
    seen = set()
    for f in sorted(files, key=os.path.getmtime, reverse=True):
        norm = os.path.normpath(f)
        if norm not in seen:
            seen.add(norm)
            uniq.append(f)
    return uniq

def read_file(path):
    if path.endswith('.xlsx'):
        return pd.read_excel(path)
    return pd.read_csv(path)

def load_many(files, label):
    frames = []
    for path in files:
        try:
            frames.append(read_file(path))
            print(f"[LOAD:{label}] {path}")
        except Exception as e:
            print(f"[WARN:{label}] load failed: {path} ({e})")
    if not frames:
        return pd.DataFrame()
    return pd.concat(frames, ignore_index=True)

def normalize_key_series(s):
    return s.astype(str).str.strip().str.lower()

def choose_best_key_pair(df_left, left_candidates, df_right, right_candidates):
    best = None
    best_overlap = -1
    for l_col in left_candidates:
        if l_col not in df_left.columns:
            continue
        l_set = set(normalize_key_series(df_left[l_col]).dropna().tolist())
        if not l_set:
            continue
        for r_col in right_candidates:
            if r_col not in df_right.columns:
                continue
            r_set = set(normalize_key_series(df_right[r_col]).dropna().tolist())
            if not r_set:
                continue
            overlap = len(l_set & r_set)
            if overlap > best_overlap:
                best_overlap = overlap
                best = (l_col, r_col, overlap)
    return best

# --- [ë©”ì¸ ë°°í¬ ì—”ì§„] ---
def build_final_reports(external_config=None):
    print("[INFO] ë¦¬í¬íŠ¸ ë¹Œë“œ ì—”ì§„ ê°€ë™...")
    
    # 1. íŒŒì¼ ìë™ íƒìƒ‰ (í‘œì¤€ ë°ì´í„° -> sales_raw í´ë” -> ë£¨íŠ¸ ìˆœì„œ)
    # ì‹¤ì  ë°ì´í„° ê²€ìƒ‰ (ìƒŒë“œë°•ìŠ¤ ë³‘í•©ë³¸ standardized_sales ìš°ì„ )
    sales_search_paths = [
        'output/processed_data/standardized_sales_*.csv',
        'output/processed_data/standardized_sales.csv',
        'data/sales/standardized_sales.csv',
        'standardized_sales.csv',
    ]
    sales_fallback_paths = [
        'data/sales/*.xlsx',
        'data/sales/*.csv',
        '*sales*.csv',
        '*sales*.xlsx'
    ]
    
    sales_files = list_files(sales_search_paths)
    use_standardized_sales = len(sales_files) > 0
    if use_standardized_sales:
        sales_files = [sales_files[0]]
    if not sales_files:
        sales_files = list_files(sales_fallback_paths)
        use_standardized_sales = False

    if not sales_files:
        print("[ERROR] ì‹¤ì  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ëª©í‘œ ë°ì´í„° ê²€ìƒ‰
    target_search_paths = [
        'data/targets/*target*.csv',
        'data/targets/*target*.xlsx',
        'data/targets/*ëª©í‘œ*.csv',
        'data/targets/*ëª©í‘œ*.xlsx',
        'data/targets/*.csv',
        'data/targets/*.xlsx',
        '*target*.csv',
        '*target*.xlsx'
    ]
    
    target_files = list_files(target_search_paths)
    if not target_files:
        print("[ERROR] ëª©í‘œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    crm_search_paths = [
        'data/crm/*.xlsx',
        'data/crm/*.csv',
        '*crm*.xlsx',
        '*crm*.csv'
    ]
    crm_files = [] if use_standardized_sales else list_files(crm_search_paths)

    print(f"[INFO] ì‹¤ì  íŒŒì¼ ìˆ˜: {len(sales_files)}")
    print(f"[INFO] KPI ëª©í‘œ íŒŒì¼ ìˆ˜: {len(target_files)}")
    print(f"[INFO] CRM íŒŒì¼ ìˆ˜: {len(crm_files)}")
    if use_standardized_sales:
        print("[INFO] standardized_sales ë³‘í•©ë³¸ì„ ê¸°ì¤€ ë°ì´í„°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    df_raw = load_many(sales_files, 'SALES')
    df_targets = load_many(target_files, 'TARGETS')
    df_crm = load_many(crm_files, 'CRM') if crm_files else pd.DataFrame()
    
    # 0. ë™ì  ë§¤í•‘ ì„¤ì • ë¡œë“œ
    mapping_config = load_mapping_config()

    # 1. ì»¬ëŸ¼ ë§¤í•‘ ë° í‘œì¤€í™”
    df_raw = auto_map_columns(df_raw, mapping_config)
    df_targets = auto_map_columns(df_targets, mapping_config)
    if not df_crm.empty:
        df_crm = auto_map_columns(df_crm, mapping_config)

    # ë°ì´í„° í—¬ìŠ¤ ì²´í¬ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    data_health = {
        'mapped_fields': {},
        'missing_fields': [],
        'integrity_score': 100
    }

    # ë§¤í•‘ ìƒíƒœ ê¸°ë¡
    for std_col in mapping_config.keys():
        if std_col in df_raw.columns: data_health['mapped_fields'][f"Sales_{std_col}"] = "OK"
        if std_col in df_targets.columns: data_health['mapped_fields'][f"Target_{std_col}"] = "OK"

    # ëˆ„ë½ëœ ì»¬ëŸ¼ ì²˜ë¦¬ ë° ë°ì´í„° ì •ì œ (Essential: ì§€ì , ì„±ëª…, í’ˆëª©, ëª©í‘œê¸ˆì•¡)
    for col in ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']:
        if col in df_raw.columns:
            df_raw[col] = df_raw[col].astype(str).str.strip()
        if col in df_targets.columns:
            df_targets[col] = df_targets[col].astype(str).str.strip()
        if not df_crm.empty and col in df_crm.columns:
            df_crm[col] = df_crm[col].astype(str).str.strip()

    if 'ì²˜ë°©ê¸ˆì•¡' in df_raw.columns:
        df_raw['ì²˜ë°©ê¸ˆì•¡'] = pd.to_numeric(df_raw['ì²˜ë°©ê¸ˆì•¡'], errors='coerce').fillna(0)
    if 'ëª©í‘œê¸ˆì•¡' in df_targets.columns:
        df_targets['ëª©í‘œê¸ˆì•¡'] = pd.to_numeric(df_targets['ëª©í‘œê¸ˆì•¡'], errors='coerce').fillna(0)

    essential_cols = ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©', 'ëª©í‘œê¸ˆì•¡']
    for col in essential_cols:
        target_df_col = col if col in df_targets.columns else None
        if target_df_col:
            df_targets[col] = df_targets[col].astype(str).str.strip() if col != 'ëª©í‘œê¸ˆì•¡' else pd.to_numeric(df_targets[col], errors='coerce').fillna(0)
        else:
            data_health['missing_fields'].append(f"Target_{col}")
            df_targets[col] = 'Unknown' if col != 'ëª©í‘œê¸ˆì•¡' else 0
            data_health['integrity_score'] -= 15

    # ì‹¤ì  ë°ì´í„° í•„ë“œ ì²´í¬ (HIR, PHR ë“±ì„ ìœ„í•œ í•„ë“œë“¤)
    for col in ['activities', 'segment', 'ë‚ ì§œ', 'ì²˜ë°©ìˆ˜ëŸ‰']:
        if col in df_raw.columns:
            data_health['mapped_fields'][col] = col
        else:
            data_health['missing_fields'].append(col)
            # ê¸°ë³¸ê°’ ì±„ìš°ê¸° (ì—°ì‚° ì˜¤ë¥˜ ë°©ì§€)
            from datetime import datetime
            if col == 'activities': df_raw[col] = 'General'
            if col == 'segment': df_raw[col] = 'Normal'
            if col == 'ë‚ ì§œ': df_raw[col] = pd.to_datetime(datetime.now().strftime('%Y-%m-%d'))
            if col == 'ì²˜ë°©ìˆ˜ëŸ‰':
                if 'ì²˜ë°©ê¸ˆì•¡' in df_raw.columns:
                    df_raw['ì²˜ë°©ìˆ˜ëŸ‰'] = (df_raw['ì²˜ë°©ê¸ˆì•¡'] / 1000).astype(int)
                else:
                    df_raw['ì²˜ë°©ìˆ˜ëŸ‰'] = 0
            data_health['integrity_score'] -= 10
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì›”(Month) íŒŒì‹± â€“ ë‹¨ì¼ í—¬í¼ë¡œ í†µí•©í•˜ì—¬ ì¤‘ë³µ ì œê±°
    # auto_map_columnsì´ 'ëª©í‘œì›”' â†’ 'ì›”'ë¡œ ì´ë¦„ë§Œ ë°”ê¾¸ë¯€ë¡œ
    # ê°’ì´ '2026-01' ë¬¸ìì—´ì¸ ê²½ìš°ê°€ ìˆìŒ. ì´ë¥¼ ë°˜ë“œì‹œ ìˆ«ìë¡œ ë³€í™˜.
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def parse_month_col(df):
        """df ë‚´ì—ì„œ ì›”(ì •ìˆ˜ 1-12)ì„ ì¶”ì¶œí•´ ë°˜í™˜í•œë‹¤."""
        # ìš°ì„ ìˆœìœ„: 'ì›”' â†’ 'ëª©í‘œì›”' â†’ 'ë‚ ì§œ' â†’ 'í™œë™ì¼ì'
        for src in ['ì›”', 'ëª©í‘œì›”', 'ë‚ ì§œ', 'í™œë™ì¼ì']:
            if src not in df.columns:
                continue
            s = df[src]
            # ì´ë¯¸ ì •ìˆ˜í˜•ì´ë©´ ë°”ë¡œ ë°˜í™˜
            if s.dtype in ['int32', 'int64']:
                return s
            # ë‚ ì§œ/ë¬¸ìì—´ íŒŒì‹± ì‹œë„ (ì˜ˆ: '2026-01', '2026-01-15' ë“±)
            parsed = pd.to_datetime(s, errors='coerce').dt.month
            if parsed.notna().sum() > len(df) * 0.5:   # ì ˆë°˜ ì´ìƒ íŒŒì‹± ì„±ê³µ ì‹œ ì±„íƒ
                return parsed
            # ìˆ«ìë§Œ ì¶”ì¶œ ì‹œë„ (ì˜ˆ: '1', '01', '2026-01' â†’ '2026' â†’ ì›” ì•„ë‹˜,ê·¸ëƒ¥ skip)
            numeric = pd.to_numeric(s, errors='coerce')
            valid = numeric[(numeric >= 1) & (numeric <= 12)]
            if len(valid) > len(df) * 0.5:
                return numeric
        return pd.Series([1] * len(df), index=df.index)

    df_raw['ì›”']     = parse_month_col(df_raw).fillna(1).astype(int)
    df_targets['ì›”'] = parse_month_col(df_targets).fillna(1).astype(int)
    if not df_crm.empty:
        df_crm['ì›”'] = parse_month_col(df_crm).fillna(1).astype(int)

    print(f"DEBUG: Sales month dist  â†’ {df_raw['ì›”'].value_counts().sort_index().to_dict()}")
    print(f"DEBUG: Target month dist â†’ {df_targets['ì›”'].value_counts().sort_index().to_dict()}")
    if not df_crm.empty:
        print(f"DEBUG: CRM month dist    â†’ {df_crm['ì›”'].value_counts().sort_index().to_dict()}")

    # ê¸°ë³¸ ì»¬ëŸ¼ í™•ì¸ (ì²˜ë°©ìˆ˜ëŸ‰ ë“±)
    if 'ì²˜ë°©ìˆ˜ëŸ‰' not in df_raw.columns:
        df_raw['ì²˜ë°©ìˆ˜ëŸ‰'] = (df_raw['ì²˜ë°©ê¸ˆì•¡'] / 1000).astype(int) if 'ì²˜ë°©ê¸ˆì•¡' in df_raw.columns else 0

    # ë§¤ì¹­ í‚¤ ìë™ ì •í•©: ì´ë¦„/ID ì¤‘ ê²¹ì¹¨ì´ í° ì»¬ëŸ¼ ì¡°í•©ì„ ì„ íƒ
    branch_pair = choose_best_key_pair(df_raw, ['ì§€ì ', 'ì§€ì ID', 'ì§€ì ëª…'], df_targets, ['ì§€ì ', 'ì§€ì ID', 'ì§€ì ëª…'])
    rep_pair = choose_best_key_pair(df_raw, ['ì„±ëª…', 'ë‹´ë‹¹ìëª…', 'ë‹´ë‹¹ìID'], df_targets, ['ì„±ëª…', 'ë‹´ë‹¹ìëª…', 'ë‹´ë‹¹ìID'])
    prod_pair = choose_best_key_pair(df_raw, ['í’ˆëª©', 'í’ˆëª©ëª…', 'í’ˆëª©ID'], df_targets, ['í’ˆëª©', 'í’ˆëª©ëª…', 'í’ˆëª©ID'])

    if not branch_pair or not rep_pair or not prod_pair:
        print("[WARN] í‚¤ ìë™ ì •í•© ì‹¤íŒ¨: ê¸°ë³¸ í‚¤(ì§€ì /ì„±ëª…/í’ˆëª©)ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.")
        df_raw['__k_branch'] = normalize_key_series(df_raw.get('ì§€ì ', ''))
        df_raw['__k_rep'] = normalize_key_series(df_raw.get('ì„±ëª…', ''))
        df_raw['__k_prod'] = normalize_key_series(df_raw.get('í’ˆëª©', ''))
        df_targets['__k_branch'] = normalize_key_series(df_targets.get('ì§€ì ', ''))
        df_targets['__k_rep'] = normalize_key_series(df_targets.get('ì„±ëª…', ''))
        df_targets['__k_prod'] = normalize_key_series(df_targets.get('í’ˆëª©', ''))
    else:
        b_l, b_r, b_ov = branch_pair
        r_l, r_r, r_ov = rep_pair
        p_l, p_r, p_ov = prod_pair
        print(f"[INFO] í‚¤ ë§¤ì¹­ ì„ íƒ: branch({b_l}<->{b_r}, {b_ov}), rep({r_l}<->{r_r}, {r_ov}), prod({p_l}<->{p_r}, {p_ov})")
        df_raw['__k_branch'] = normalize_key_series(df_raw[b_l])
        df_raw['__k_rep'] = normalize_key_series(df_raw[r_l])
        df_raw['__k_prod'] = normalize_key_series(df_raw[p_l])
        df_targets['__k_branch'] = normalize_key_series(df_targets[b_r])
        df_targets['__k_rep'] = normalize_key_series(df_targets[r_r])
        df_targets['__k_prod'] = normalize_key_series(df_targets[p_r])

    # CRM í™œë™ëª…ì„ ì‹¤ì  ë°ì´í„°(activity)ë¡œ ë§¤í•‘
    if not df_crm.empty and 'activities' in df_crm.columns:
        for col in ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']:
            if col not in df_crm.columns:
                df_crm[col] = 'Unknown'
        df_crm['activities'] = df_crm['activities'].astype(str).str.strip()
        df_crm = df_crm[df_crm['activities'].notna() & (df_crm['activities'] != '')].copy()

        weight_col = None
        for c in ['í™˜ì‚°ì½œ(Weighted)', 'ì½œìˆ˜', 'ê°€ì¤‘ì¹˜']:
            if c in df_crm.columns:
                weight_col = c
                break
        if weight_col:
            df_crm['act_weight'] = pd.to_numeric(df_crm[weight_col], errors='coerce').fillna(1.0)
        else:
            df_crm['act_weight'] = 1.0

        act_keys = ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©', 'ì›”']
        crm_activity = (
            df_crm.groupby(act_keys + ['activities'])['act_weight']
            .sum()
            .reset_index()
            .sort_values(act_keys + ['act_weight'], ascending=[True, True, True, True, False])
            .drop_duplicates(subset=act_keys)
            [act_keys + ['activities']]
        )

        if 'activities' not in df_raw.columns:
            df_raw['activities'] = np.nan
        df_raw = df_raw.merge(crm_activity, on=act_keys, how='left', suffixes=('', '_crm'))
        if 'activities_crm' in df_raw.columns:
            df_raw['activities'] = np.where(
                df_raw['activities_crm'].notna() & (df_raw['activities_crm'].astype(str).str.strip() != ''),
                df_raw['activities_crm'],
                df_raw['activities']
            )
            df_raw = df_raw.drop(columns=['activities_crm'])

        mapped_activity_count = int(df_raw['activities'].notna().sum())
        print(f"DEBUG: CRM activity mapped rows â†’ {mapped_activity_count:,}")
        data_health['mapped_fields']['activities'] = "CRM.activities"
        if 'activities' in data_health['missing_fields']:
            data_health['missing_fields'] = [x for x in data_health['missing_fields'] if x != 'activities']
            
    # ê°€ì¤‘ì¹˜ ì„¤ì • (ìŠ¬ë¼ì´ë” ê°’ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ì—‘ì…€ì—ì„œ ë¡œë“œ)
    if external_config:
        W_ACT = external_config.get('hir_weights', {})
        W_SEG = external_config.get('pi_weights', {})
        print("[INFO] ì™¸ë¶€ ì„¤ì •(Streamlit ìŠ¬ë¼ì´ë”) ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.")
        T_MEAN, T_STD = 70.0, 10.0
    else:
        # ë§ˆìŠ¤í„° ë¡œì§ íŒŒì¼ ê²½ë¡œ ìˆ˜ì •
        logic_path = 'data/logic/SFE_Master_Logic_v1.0.xlsx'
        if not os.path.exists(logic_path):
            logic_path = 'SFE_Master_Logic_v1.0.xlsx' # ë£¨íŠ¸ í™•ì¸
            
        xl = pd.ExcelFile(logic_path)
        W_ACT = dict(zip(xl.parse('Activity_Weights')['í™œë™ëª…'], xl.parse('Activity_Weights')['ê°€ì¤‘ì¹˜']))
        W_SEG = dict(zip(xl.parse('Segment_Weights')['ë³‘ì›ê·œëª¨'], xl.parse('Segment_Weights')['ë³´ì •ê³„ìˆ˜']))
        
        try:
            sys_setup = xl.parse('System_Setup')
            T_MEAN = float(sys_setup.loc[sys_setup['ì„¤ì •í•­ëª©'].str.contains('T-Score í‰ê· ', na=False), 'ì„¤ì •ê°’'].values[0])
            T_STD = float(sys_setup.loc[sys_setup['ì„¤ì •í•­ëª©'].str.contains('T-Score í¸ì°¨', na=False), 'ì„¤ì •ê°’'].values[0])
        except Exception as e:
            print(f"[WARN] T-Score ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            T_MEAN, T_STD = 70.0, 10.0

    # 2. ì§€í‘œ ì—°ì‚° ë° 8ëŒ€ í–‰ë™ Atomic íŒŒì‹±
    # W_ACT ë”•ì…”ë„ˆë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ activities ì»¬ëŸ¼ì˜ ë¬¸ì¥ì„ íŒŒì‹±í•´ì„œ ë¹ˆë„/ê°€ì¤‘ì¹˜ ì²´í¬
    w_act_map = {str(k).strip(): v for k, v in W_ACT.items()}
    df_raw['activities'] = df_raw['activities'].astype(str).str.strip()
    
    # ê° row (ë°©ë¬¸/Call) ë³„ë¡œ 8ëŒ€ í–‰ë™ ì ìˆ˜ ë§¤í•‘ (Atomic Split)
    for b in ATOMIC_BEHAVIORS:
        # activities ë‚´ì— í•´ë‹¹ ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ìˆìœ¼ë©´ 1.0 (ë˜ëŠ” í•´ë‹¹ ê°€ì¤‘ì¹˜), ì•„ë‹ˆë©´ 0.0
        df_raw[b] = df_raw['activities'].apply(lambda x: 1.0 if b in x else 0.0)
    
    # ì „ì²´ HIR ì—°ì‚°ì„ ìœ„í•´: 8ëŒ€ í–‰ë™ * ê°€ì¤‘ì¹˜ì˜ í•©
    df_raw['HIR_W'] = 0.0
    for b in ATOMIC_BEHAVIORS:
        weight = float(w_act_map.get(b, 1.0))
        df_raw['HIR_W'] += df_raw[b] * weight
        
    df_raw['SEG_W'] = df_raw['segment'].map(W_SEG).fillna(1.0)

    # RTR: ë‚ ì§œ_ts ê°ì‡  ë¡œì§ $exp(-0.035 \times t)$
    df_raw['ë‚ ì§œ_ts'] = pd.to_datetime(df_raw['ë‚ ì§œ'], errors='coerce')
    current_time = pd.Timestamp.now()
    t_days = (current_time - df_raw['ë‚ ì§œ_ts']).dt.days.clip(lower=0)
    df_raw['RTR_raw'] = np.exp(-0.035 * t_days).fillna(0)

    print(f"DEBUG: df_raw shape: {df_raw.shape}")
    print(f"DEBUG: df_raw columns: {df_raw.columns.tolist()}")

    group_cols = ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©', '__k_branch', '__k_rep', '__k_prod']
    
    # ê° ê·¸ë£¹ë³„ë¡œ Atomic 8 í–‰ë™ ì´í•© ê³„ì‚°
    atomic_agg_dict = {b: 'sum' for b in ATOMIC_BEHAVIORS}
    agg_dict = {'ì²˜ë°©ê¸ˆì•¡': 'sum', 'ì²˜ë°©ìˆ˜ëŸ‰': 'sum', 'HIR_W': 'mean', 'RTR_raw': 'mean'}
    agg_dict.update(atomic_agg_dict)
    
    actual_agg = df_raw.groupby(group_cols).agg(agg_dict).reset_index()
    print(f"DEBUG: actual_agg shape: {actual_agg.shape}")

    # BCR: ë°©ë¬¸ ê°„ê²© í‘œì¤€í¸ì°¨ $\sigma$ ìœ ë„. ì¼ê´€ì„±ì´ ë†’ìœ¼ë©´ í‘œì¤€í¸ì°¨ ë‚®ìŒ
    df_sorted = df_raw.sort_values(group_cols + ['ë‚ ì§œ_ts'])
    df_sorted['interval'] = df_sorted.groupby(group_cols)['ë‚ ì§œ_ts'].diff().dt.days
    # ì—­ìˆ˜ë¡œ í•´ì„œ ê°’ì´ í´ìˆ˜ë¡(ê·œì¹™ì ì¼ìˆ˜ë¡) ì¢‹ê²Œ êµ¬ì„± (interval stdê°€ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
    # 0 ë¶„ëª¨ ë°©ì§€ ìœ„í•´ + 1
    bcr_raw = df_sorted.groupby(group_cols)['interval'].apply(lambda x: 1.0 / (np.std(x) + 1.0) if len(x) > 1 else 0).reset_index(name='BCR_raw')

    hir_raw = df_raw.groupby(group_cols).apply(lambda x: (x['HIR_W'] * x['SEG_W']).sum() / len(x) if len(x)>0 else 0, include_groups=False).reset_index(name='HIR_raw')
    df_master = pd.merge(actual_agg, hir_raw, on=group_cols)
    df_master = pd.merge(df_master, bcr_raw, on=group_cols, how='left')
    
    # ë§ˆìŠ¤í„° ì‹œíŠ¸ì—ì„œ ê°€ì ¸ì˜¨ T_MEAN, T_STD ë¡œ ê°€ì¤‘í‰ê·  í™˜ì‚° (T-Score)
    df_master['HIR'] = t_score(df_master['HIR_raw'].values, T_MEAN, T_STD)
    df_master['RTR'] = t_score(df_master['RTR_raw'].values, T_MEAN, T_STD)
    df_master['BCR'] = t_score(df_master['BCR_raw'].values, T_MEAN, T_STD)
    df_master['PHR'] = np.full_like(df_master['HIR'].values, T_MEAN)

    # standardized_salesì— ê¸°ì¡´ ì§€í‘œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    # standardized_salesì— ê¸°ì¡´ ì§€í‘œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    for metric in ['HIR', 'RTR', 'BCR', 'PHR']:
        target_col = None
        if f"{metric}_Raw" in df_raw.columns:
            target_col = f"{metric}_Raw"
        elif metric in df_raw.columns:
            target_col = metric
            
        if target_col:
            metric_df = df_raw[group_cols + [target_col]].copy()
            metric_df[target_col] = pd.to_numeric(metric_df[target_col], errors='coerce')
            metric_agg = metric_df.groupby(group_cols)[target_col].mean().reset_index(name=f'{metric}_src')
            df_master = df_master.merge(metric_agg, on=group_cols, how='left')
            src = df_master[f'{metric}_src']
            if src.notna().sum() > 0:
                # If values are raw (e.g. 0~5), apply t_score. If already scaled (like 0-100), just use them.
                # Usually standard raw values have small stdev
                if (src.std() or 0) > 0:
                    df_master[metric] = t_score(src.fillna(src.mean()).values, T_MEAN, T_STD)
                else:
                    df_master[metric] = np.full_like(src, T_MEAN) # ê¸°ë³¸ ì ìˆ˜
            df_master = df_master.drop(columns=[f'{metric}_src'])

    # ëª©í‘œ ë§¤ì¹­ ë° ëˆ„ë½ ì²´í¬
    df_targets_agg = df_targets.groupby(['__k_branch','__k_rep','__k_prod'])['ëª©í‘œê¸ˆì•¡'].sum().reset_index()
    df_final = pd.merge(df_master, df_targets_agg, on=['__k_branch','__k_rep','__k_prod'], how='left')
    
    # ëˆ„ë½ ë°ì´í„° ì¶”ì¶œ (ì‹¤ì ì€ ìˆìœ¼ë‚˜ ëª©í‘œê°€ ì—†ëŠ” ê²½ìš°)
    missing_targets_df = df_final[df_final['ëª©í‘œê¸ˆì•¡'].isna() | (df_final['ëª©í‘œê¸ˆì•¡'] == 0)]
    missing_log = missing_targets_df[['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']].to_dict('records')
    
    df_final = df_final.fillna(0)
    df_final['ë‹¬ì„±ë¥ '] = np.where(df_final['ëª©í‘œê¸ˆì•¡'] > 0, (df_final['ì²˜ë°©ê¸ˆì•¡'] / df_final['ëª©í‘œê¸ˆì•¡']) * 100, 0)
    
    print(f"DEBUG: df_raw shape: {df_raw.shape}")
    print(f"DEBUG: actual_agg shape: {actual_agg.shape}")
    print(f"DEBUG: df_final shape: {df_final.shape}")
    
    if df_final.empty:
        print("[CRITICAL] df_final is empty. There is no matching data between sales and targets.")

    # --- [ì½”ì¹­ ë£° ì—”ì§„] ---
    def get_coaching_message(hir, rtr, bcr, ach, th_hir=70, th_rtr=70, th_bcr=70, th_ach=100):
        # ë§ˆìŠ¤í„° ë¡œì§ ì½”ì¹­ ë£° 
        if ach >= th_ach:
            if hir >= th_hir and rtr >= th_rtr:
                return "The Masterclass", "í˜„ì¬ì˜ ë†’ì€ í™œë™ëŸ‰ê³¼ ìš°ìˆ˜í•œ ê´€ê³„ ìœ ì§€ ëŠ¥ë ¥ì„ ìœ ì§€í•˜ì„¸ìš”. Best Practice ì‚¬ë¡€ë¡œ ê³µìœ ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
            elif hir < th_hir and rtr >= th_rtr:
                return "The Relationship Builder", "ê³ ê°ê³¼ì˜ ê´€ê³„ëŠ” í›Œë¥­í•˜ë‚˜ í™œë™ëŸ‰ì´ ë‹¤ì†Œ ë¶€ì¡±í•©ë‹ˆë‹¤. ë°©ë¬¸ ì»¤ë²„ë¦¬ì§€ë¥¼ ëŠ˜ë ¤ íŒŒì´í”„ë¼ì¸ì„ í™•ì¥í•˜ì„¸ìš”."
            elif hir >= th_hir and rtr < th_rtr:
                return "The Volume Driver", "í™œë™ëŸ‰ì€ ìš°ìˆ˜í•˜ë‚˜ ê´€ê³„ ê¹Šì´ê°€ ì•„ì‰½ìŠµë‹ˆë‹¤. í•µì‹¬ ê³ ê°ì¸µì— ëŒ€í•œ ì‹¬ì¸µì ì´ê³  í€„ë¦¬í‹° ë†’ì€ ë””í…Œì¼ë§ì´ í•„ìš”í•©ë‹ˆë‹¤."
            else:
                return "The Lucky Star", "ë°ì´í„°ìƒ ìœ íš¨í–‰ë™ê³¼ ê´€ê³„ì˜¨ë„ê°€ ë‚®ìŒì—ë„ ëª©í‘œë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì™¸ë¶€ ìš”ì¸(ì‹œì¥ ìƒí™© ë“±)ì´ë‚˜ ì¼íšŒì„± ë§¤ì¶œ ì—¬ë¶€ë¥¼ ì ê²€í•˜ì„¸ìš”."
        else:
            if hir >= th_hir and bcr < th_bcr:
                return "The Erratic Sprinter", "í™œë™ëŸ‰ì€ ë§ìœ¼ë‚˜ ë°©ë¬¸ì´ ë¶ˆê·œì¹™í•©ë‹ˆë‹¤. ì‚¬ì „ ê³„íš(PHR)ì„ ì² ì €íˆ ê¸°íší•˜ì—¬ ê· ì¼í•˜ê²Œ ë°©ë¬¸ ì¼ì •ì„ ì•ˆë°°í•˜ì„¸ìš”."
            elif rtr < th_rtr and bcr >= th_bcr:
                return "The Routine Visitor", "ê·œì¹™ì ìœ¼ë¡œ ê¾¸ì¤€íˆ ë°©ë¬¸í•˜ë‚˜ ê³ ê°ê³¼ì˜ ê´€ê³„ ì˜¨ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ë‹¨ìˆœ ì œí’ˆ ì „ë‹¬ì„ ë„˜ì–´ì„  ì†”ë£¨ì…˜ ì œì•ˆ(PT/ë‹ˆì¦ˆí™˜ê¸°) ìŠ¤í‚¬ êµìœ¡ì´ ì‹œê¸‰í•©ë‹ˆë‹¤."
            elif hir < th_hir and bcr < th_bcr:
                return "The Ghost Hunter", "í™œë™ëŸ‰ê³¼ ê·œì¹™ì„± ëª¨ë‘ ì €ì¡°í•©ë‹ˆë‹¤. ê·¼íƒœ ë° ì¼ì¼ í™œë™ ê³„íšì— ëŒ€í•œ ë°€ì°© ì½”ì¹­ê³¼ íŒŒì´í”„ë¼ì¸ ì „ë©´ ì¬ì„¤ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤."
            else:
                return "The Hard Worker", "ì„±ì‹¤í•˜ê²Œ ì–‘ì§ˆì˜ í™œë™ì„ ìˆ˜í–‰í•˜ê³  ìˆìœ¼ë‚˜ ì„±ê³¼ë¡œ ì´ì–´ì§€ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤. íƒ€ê²ŸíŒ…(Segment)ì´ë‚˜ ì£¼ë ¥ í’ˆëª©(MS) ì „ëµì˜ ì¬ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤."

    # 3. JSON ë°ì´í„° íŠ¸ë¦¬ êµ¬ì¶•
    hierarchy = {
        'branches': {}, 
        'products': sorted(df_final['í’ˆëª©'].unique().tolist()), 
        'total_avg': df_final[['HIR', 'RTR', 'BCR', 'PHR']].mean().to_dict(),
        'missing_data': missing_log, # ëˆ„ë½ëœ ë ˆì½”ë“œ ì •ë³´
        'data_health': data_health   # í•„ë“œ ë§¤í•‘ í—¬ìŠ¤ ì²´í¬ ì •ë³´ ì¶”ê°€
    }
    
    month_axis = list(range(1, 13))

    # íƒ€ê²Ÿ ì›” ë°ì´í„°ëŠ” ë§¤ì¹­ í‚¤ë¡œ sales ë¼ë²¨ì— ë§¤í•‘í•œ ë’¤ ì‚¬ìš©
    target_monthly = (
        df_targets[['__k_branch', '__k_rep', '__k_prod', 'ì›”', 'ëª©í‘œê¸ˆì•¡']]
        .merge(
            df_final[['__k_branch', '__k_rep', '__k_prod', 'ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']].drop_duplicates(),
            on=['__k_branch', '__k_rep', '__k_prod'],
            how='inner'
        )
    )

    # 2.5 ëŒ€í‘œ(Rep) ë ˆë²¨ ì§€í‘œ ì •ê·œí™” (ë³€ë³„ë ¥ í™•ë³´)
    # ê°œë³„ í’ˆëª© T-scoreì˜ í‰ê· ì„ ì“°ë©´ ë³€ë³„ë ¥ì´ ì‚¬ë¼ì§€ë¯€ë¡œ(í‰ê· íšŒê·€), Rep ë ˆë²¨ì—ì„œ Raw ì ìˆ˜ë¥¼ ë‹¤ì‹œ T-scoreí™”
    df_rep_raw_calc = df_final.groupby(['ì§€ì ', 'ì„±ëª…']).agg({
        'HIR_raw': 'mean',
        'RTR_raw': 'mean',
        'BCR_raw': 'mean',
        'ì²˜ë°©ê¸ˆì•¡': 'sum',
        'ëª©í‘œê¸ˆì•¡': 'sum'
    }).reset_index()
    
    df_rep_raw_calc['REP_HIR'] = t_score(df_rep_raw_calc['HIR_raw'].values, T_MEAN, T_STD)
    df_rep_raw_calc['REP_RTR'] = t_score(df_rep_raw_calc['RTR_raw'].values, T_MEAN, T_STD)
    df_rep_raw_calc['REP_BCR'] = t_score(df_rep_raw_calc['BCR_raw'].values, T_MEAN, T_STD)
    df_rep_raw_calc['REP_ACH'] = (df_rep_raw_calc['ì²˜ë°©ê¸ˆì•¡'] / df_rep_raw_calc['ëª©í‘œê¸ˆì•¡'] * 100).fillna(0)
    
    # ë™ì  Threshold (ì¤‘ì•™ê°’) ê³„ì‚°
    th_hir = float(df_rep_raw_calc['REP_HIR'].median())
    th_rtr = float(df_rep_raw_calc['REP_RTR'].median())
    th_bcr = float(df_rep_raw_calc['REP_BCR'].median())
    # ë‹¬ì„±ë¥ ì€ 100%ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ë˜, ì „ë°˜ì ìœ¼ë¡œ ë‚®ìœ¼ë©´ ì¤‘ì•™ê°’ ì‚¬ìš© ê³ ë ¤ ê°€ëŠ¥í•˜ë‚˜ ì¼ë‹¨ 100 ìœ ì§€ ë˜ëŠ” í•˜í–¥ ì¡°ì •
    th_ach = 100.0 if df_rep_raw_calc['REP_ACH'].max() >= 100 else float(df_rep_raw_calc['REP_ACH'].median())
    
    print(f"DEBUG: Coaching Thresholds -> HIR:{th_hir:.1f}, RTR:{th_rtr:.1f}, BCR:{th_bcr:.1f}, ACH:{th_ach:.1f}")

    for br in df_final['ì§€ì '].unique():
        df_br = df_final[df_final['ì§€ì '] == br]
        hierarchy['branches'][br] = {
            'members': [],
            'avg': df_br[['HIR', 'RTR', 'BCR', 'PHR']].mean().to_dict(),
            'achieve': calc_achieve(df_br['ì²˜ë°©ê¸ˆì•¡'].sum(), df_br['ëª©í‘œê¸ˆì•¡'].sum()),
            'monthly_actual': df_raw[df_raw['ì§€ì '] == br].groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
            'monthly_target': target_monthly[target_monthly['ì§€ì '] == br].groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
            'analysis': run_full_analysis(df_br),
            'prod_analysis': {pd: {
                'analysis': run_full_analysis(df_br[df_br['í’ˆëª©']==pd]),
                'achieve': calc_achieve(df_br[df_br['í’ˆëª©']==pd]['ì²˜ë°©ê¸ˆì•¡'].sum(), df_br[df_br['í’ˆëª©']==pd]['ëª©í‘œê¸ˆì•¡'].sum()),
                'avg': df_br[df_br['í’ˆëª©']==pd][['HIR','RTR','BCR','PHR']].mean().to_dict()
            } for pd in hierarchy['products']}
        }
        
        for rep in df_br['ì„±ëª…'].unique():
            df_rep = df_br[df_br['ì„±ëª…'] == rep]
            rep_analysis = run_full_analysis(df_rep)
            if rep_analysis is not None:
                real_shap = {k: float(v) for k, v in rep_analysis['importance'].items()}
            else:
                real_shap = {b: np.nan for b in ATOMIC_BEHAVIORS}
            
            prod_matrix = []
            rep_raw = df_raw[(df_raw['ì§€ì '] == br) & (df_raw['ì„±ëª…'] == rep)]
            total_sales = float(rep_raw['ì²˜ë°©ê¸ˆì•¡'].sum()) if not rep_raw.empty else 0.0
            if total_sales > 0 and not rep_raw.empty:
                max_m = rep_raw['ì›”'].max()
                prev_m = max_m - 1
                for pd_name in hierarchy['products']:
                    p_data = rep_raw[rep_raw['í’ˆëª©'] == pd_name]
                    p_sales = float(p_data['ì²˜ë°©ê¸ˆì•¡'].sum())
                    ms = (p_sales / total_sales * 100) if total_sales else 0.0
                    cm_sales = float(p_data[p_data['ì›”'] == max_m]['ì²˜ë°©ê¸ˆì•¡'].sum())
                    pm_sales = float(p_data[p_data['ì›”'] == prev_m]['ì²˜ë°©ê¸ˆì•¡'].sum())
                    if pm_sales > 0:
                        growth = ((cm_sales - pm_sales) / pm_sales) * 100
                    else:
                        growth = 100.0 if cm_sales > 0 else 0.0
                    prod_matrix.append({'name': pd_name, 'ms': ms, 'growth': growth})
            else:
                prod_matrix = [{'name': pd_name, 'ms': 0.0, 'growth': 0.0} for pd_name in hierarchy['products']]
            
            # 4ë¶„ë©´ ì „ëµ ê°€ì´ë“œë¼ì¸: ê¸°ì¤€ì„  ê³„ì‚° ë° ì½”ì¹­ ì•¡ì…˜ íŠ¸ë¦¬ê±°
            ms_values = [p['ms'] for p in prod_matrix if p['ms'] > 0]
            avg_ms = float(sum(ms_values) / len(ms_values)) if ms_values else 0.0
            
            # ì½”ì¹­ ë©”ì‹œì§€ ì—°ì‚°
            rep_stats = df_rep_raw_calc[df_rep_raw_calc['ì„±ëª…'] == rep].iloc[0]
            rep_hir = float(rep_stats['REP_HIR'])
            rep_rtr = float(rep_stats['REP_RTR'])
            rep_bcr = float(rep_stats['REP_BCR'])
            rep_ach = float(rep_stats['REP_ACH'])
            
            c_name, c_action = get_coaching_message(rep_hir, rep_rtr, rep_bcr, rep_ach, th_hir, th_rtr, th_bcr, th_ach)

            # Dog(Low MS / Low Growth) ë˜ëŠ” Question Mark(Low MS / High Growth) íŒŒì•… 
            # (ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ msê°€ í‰ê·  ë¯¸ë§Œì¸ ì£¼ë ¥/ë¹„ì£¼ë ¥ í’ˆëª© ì¤‘ ì˜ë¯¸ ìˆëŠ” ë³¼ë¥¨ ì¶”ì )
            weak_products = [p['name'] for p in prod_matrix if p['ms'] > 0 and p['ms'] < (avg_ms * 0.7) and p['growth'] < 0]
            if weak_products:
                c_action += f" (ğŸš¨ ì£¼ì˜: {', '.join(weak_products)} í’ˆëª©ì´ Dog ì˜ì—­ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤. í’ˆëª© ì„±ì¥ì´ ì €ì¡°í•˜ë¯€ë¡œ íƒ€ê²ŸíŒ… ì „ëµ ì¬ìˆ˜ë¦½ì´ í•„ìš”í•©ë‹ˆë‹¤.)"

            hierarchy['branches'][br]['members'].append({
                'ì„±ëª…': rep,
                'HIR': rep_hir, 'RTR': rep_rtr, 'BCR': rep_bcr, 'PHR': float(df_rep['PHR'].mean()),
                'ì²˜ë°©ê¸ˆì•¡': float(df_rep['ì²˜ë°©ê¸ˆì•¡'].sum()), 'ëª©í‘œê¸ˆì•¡': float(df_rep['ëª©í‘œê¸ˆì•¡'].sum()),
                'ì§€ì ìˆœìœ„': int(df_br.groupby('ì„±ëª…')['ì²˜ë°©ê¸ˆì•¡'].sum().rank(ascending=False)[rep]),
                'shap': real_shap,
                'coach_scenario': c_name,
                'coach_action': c_action,
                'efficiency': float(df_rep['ì²˜ë°©ê¸ˆì•¡'].sum() / (rep_hir + 1)),
                'gini': float(calc_gini(df_rep['ì²˜ë°©ê¸ˆì•¡'])),
                'avg_ms': avg_ms,
                'prod_matrix': prod_matrix,
                'monthly_actual': df_raw[(df_raw['ì§€ì ']==br) & (df_raw['ì„±ëª…']==rep)].groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
                'monthly_target': target_monthly[(target_monthly['ì§€ì ']==br) & (target_monthly['ì„±ëª…']==rep)].groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist()
            })

    hierarchy['total_prod_analysis'] = { pd: {
        'analysis': run_full_analysis(df_final[df_final['í’ˆëª©']==pd]),
        'monthly_actual': df_raw[df_raw['í’ˆëª©']==pd].groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
        'monthly_target': target_monthly[target_monthly['í’ˆëª©']==pd].groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
        'achieve': calc_achieve(df_final[df_final['í’ˆëª©']==pd]['ì²˜ë°©ê¸ˆì•¡'].sum(), df_final[df_final['í’ˆëª©']==pd]['ëª©í‘œê¸ˆì•¡'].sum()),
        'avg': df_final[df_final['í’ˆëª©']==pd][['HIR','RTR','BCR','PHR']].mean().to_dict()
    } for pd in hierarchy['products']}

    hierarchy['total'] = {
        'analysis': run_full_analysis(df_final), 'avg': hierarchy['total_avg'],
        'monthly_actual': df_raw.groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
        'monthly_target': target_monthly.groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex(month_axis, fill_value=0).tolist(),
        'achieve': calc_achieve(df_final['ì²˜ë°©ê¸ˆì•¡'].sum(), df_final['ëª©í‘œê¸ˆì•¡'].sum())
    }

    # 4. íŒŒì¼ ìƒì„±
    template_path = 'templates/report_template.html'
    if not os.path.exists(template_path):
        template_path = 'report_template.html'
        
    with open(template_path, 'r', encoding='utf-8') as f: template = f.read()
    
    class SafeEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, (np.integer, np.int64)): return int(obj)
            if isinstance(obj, (np.floating, np.float64)): 
                return float(obj) if not (np.isnan(obj) or np.isinf(obj)) else 0.0
            return super().default(obj)

    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„± (íŒŒì¼ëª… ê·œì¹™ ì ìš©)
    output_path = get_unique_filename('output', 'Strategic_Full_Dashboard', 'html')
    total_json = json.dumps(hierarchy, cls=SafeEncoder, ensure_ascii=False)
    
    # í…œí”Œë¦¿ ë‚´ì˜ ë°ì´í„° ì£¼ì… (ë”ìš± ê°•ë ¥í•œ ë§¤í•‘)
    import re
    
    # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ 'const db = /*DATA_JSON_PLACEHOLDER*/ { ... };' íŒ¨í„´ì„ ì°¾ì•„ ì „ì²´ êµì²´
    # íŒ¨í„´: 'const db = ' ë’¤ì— ì£¼ì„ í˜¹ì€ ë°ì´í„°ê°€ ì˜¤ê³  ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ì§€ì ê¹Œì§€
    pattern = r'const db = /\*DATA_JSON_PLACEHOLDER\*/ .*?;'
    replacement = f'const db = {total_json};'
    
    if re.search(pattern, template):
        template = re.sub(pattern, replacement, template)
        print("[INFO] í…œí”Œë¦¿ ë°ì´í„° ì£¼ì… ì™„ë£Œ (ì •ê·œí‘œí˜„ì‹ ë§¤ì¹­)")
    elif '/*DATA_JSON_PLACEHOLDER*/' in template:
        # ì •ê·œí‘œí˜„ì‹ì´ ì‹¤íŒ¨í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë‹¨ìˆœ ë¬¸ìì—´ êµì²´ ì‹œë„
        # í…œí”Œë¦¿ì˜ ì´ˆê¸° ê°ì²´ êµ¬ì¡°ì™€ ìƒê´€ì—†ì´ ì£¼ì„ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ êµì²´
        template = re.sub(r'/\*DATA_JSON_PLACEHOLDER\*/ .*?;', f'{total_json};', template)
        print("[INFO] í…œí”Œë¦¿ ë°ì´í„° ì£¼ì… ì™„ë£Œ (ì£¼ì„ ê¸°ì¤€ ë§¤ì¹­)")
    else:
        print("[ERROR] í…œí”Œë¦¿ì—ì„œ ë°ì´í„° ì£¼ì… ì§€ì (DATA_JSON_PLACEHOLDER)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    template = template.replace('{{BRANCH_NAME}}', 'ì „ì‚¬')
    template = template.replace('{{BRANCH_FILTER_CLASS}}', 'v-block')
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(template)
    
    # ìµœì¢… ë°ì´í„° ìƒíƒœ ìš”ì•½ ì¶œë ¥
    print("[INFO] REPORT SUMMARY:")
    print(f"   - Match Count (df_final): {len(df_final)}")
    print(f"   - Branch Count: {len(hierarchy['branches'])}")
    print(f"   - Product Count: {len(hierarchy['products'])}")
    print(f"   - Missing Targets: {len(hierarchy['missing_data'])} items")
    
    # ë§Œì•½ ë°ì´í„°ê°€ ë„ˆë¬´ ì—†ìœ¼ë©´ ê²½ê³ 
    if len(hierarchy['branches']) == 0:
        print("[WARN] No branch data generated. The report will be empty.")
    
    print(f"[OK] '{output_path}' has been created.")
    return output_path

if __name__ == "__main__":
    build_final_reports()
