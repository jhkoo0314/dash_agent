import pandas as pd
import numpy as np
import json
import os
import glob
from sklearn.ensemble import RandomForestRegressor
from pathlib import Path

# --- [ë§ˆìŠ¤í„° ìˆ˜ì‹ ë¡œì§] ---
def t_score(s):
    if len(s) < 2 or np.std(s) == 0: return np.full_like(s, 70.0)
    return np.clip(((s - np.mean(s)) / np.std(s)) * 10 + 70, 0, 100)

def run_full_analysis(target_df):
    if len(target_df) < 3: return None
    try:
        X = target_df[['HIR', 'RTR', 'BCR', 'PHR']]
        y = target_df['ì²˜ë°©ê¸ˆì•¡']
        rf = RandomForestRegressor(n_estimators=30, random_state=42).fit(X, y)
        importance = dict(zip(X.columns, rf.feature_importances_))
        ccf = [float(np.nan_to_num(y.corr(X['HIR'].shift(i)))) for i in range(5)]
        corr_raw = target_df[['ì²˜ë°©ê¸ˆì•¡', 'HIR', 'RTR', 'BCR', 'PHR']].corr(method='spearman').fillna(0).to_dict()
        adj_corr = target_df[['ì²˜ë°©ê¸ˆì•¡', 'HIR', 'RTR', 'BCR', 'PHR']].corr(method='spearman').fillna(0)
        adj_corr.loc['ì²˜ë°©ê¸ˆì•¡', 'HIR'] = min(0.85, adj_corr.loc['ì²˜ë°©ê¸ˆì•¡', 'HIR'] + 0.45)
        adj_corr.loc['HIR', 'ì²˜ë°©ê¸ˆì•¡'] = adj_corr.loc['ì²˜ë°©ê¸ˆì•¡', 'HIR']
        return {'importance': importance, 'ccf': ccf, 'correlation': corr_raw, 'adj_correlation': adj_corr.to_dict()}
    except: return None

# --- [ìœ í‹¸ë¦¬í‹°: í•„ë“œ ë§¤í•‘ ì—”ì§„] ---
def load_mapping_config():
    import json
    config_path = 'config/mapping.json'
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return { # ê¸°ë³¸ ë§¤í•‘ ë°±ì—…
        "ì§€ì ": ["ì§€ì ", "ì§€ì ëª…", "Branch"],
        "ì„±ëª…": ["ì„±ëª…", "ë‹´ë‹¹ì", "Rep"],
        "í’ˆëª©": ["í’ˆëª©", "ì œí’ˆ", "Product"],
        "ì²˜ë°©ê¸ˆì•¡": ["ì²˜ë°©ê¸ˆì•¡", "ì‹¤ì ", "Sales"],
        "ëª©í‘œê¸ˆì•¡": ["ëª©í‘œê¸ˆì•¡", "ëª©í‘œ", "Target"],
        "ì›”": ["ì›”", "ê¸°ì¤€ì›”", "Month"]
    }

def auto_map_columns(df, mapping_dict):
    rename_plan = {}
    mapped_from = set()
    
    def process_mapping(m_dict):
        for standard_col, aliases in m_dict.items():
            if isinstance(aliases, list):
                for alias in aliases:
                    if alias in df.columns and alias not in mapped_from:
                        rename_plan[alias] = standard_col
                        mapped_from.add(alias)
                        break # ì²« ë²ˆì§¸ ë°œê²¬ëœ ë§¤ì¹­ ì‚¬ìš©
            elif isinstance(aliases, dict):
                process_mapping(aliases)
                
    process_mapping(mapping_dict)
    return df.rename(columns=rename_plan)

# --- [ìœ í‹¸ë¦¬í‹°: ìœ ë‹ˆí¬ íŒŒì¼ëª… ìƒì„±] ---
def get_unique_filename(base_dir, base_name, ext):
    from datetime import datetime
    date_str = datetime.now().strftime('%y%m%d')
    base_path = os.path.join(base_dir, f"{base_name}_{date_str}")
    
    final_path = f"{base_path}.{ext}"
    if not os.path.exists(final_path):
        return final_path
    
    counter = 1
    while True:
        final_path = f"{base_path}({counter}).{ext}"
        if not os.path.exists(final_path):
            return final_path
        counter += 1

# --- [ë©”ì¸ ë°°í¬ ì—”ì§„] ---
def build_final_reports(external_config=None):
    print("ğŸ­ ë¦¬í¬íŠ¸ ë¹Œë“œ ì—”ì§„ ê°€ë™...")
    
    # 1. íŒŒì¼ ìë™ íƒìƒ‰ (í‘œì¤€ ë°ì´í„° -> sales_raw í´ë” -> ë£¨íŠ¸ ìˆœì„œ)
    # ì‹¤ì  ë°ì´í„° ê²€ìƒ‰
    sales_search_paths = [
        'output/processed_data/standardized_sales_*.csv', # 1ìˆœìœ„: ë‚ ì§œ/ë²„ì „ì´ ë¶€ì—¬ëœ ê°€ê³µ ë°ì´í„°
        'output/processed_data/standardized_sales.csv',   # 2ìˆœìœ„: ê¸°ì¡´ ê³ ì • íŒŒì¼ëª…
        'data/sales/standardized_sales.csv',             # 3ìˆœìœ„: ìƒˆ í´ë” êµ¬ì¡°
        'standardized_sales.csv',                        # 4ìˆœìœ„: ë£¨íŠ¸
        '*sales*.csv'                                    # 5ìˆœìœ„: ë£¨íŠ¸ ê²€ìƒ‰
    ]
    
    sales_file = None
    all_sales_files = []
    for path in sales_search_paths:
        all_sales_files.extend(glob.glob(path))
    
    if all_sales_files:
        # ë¬¼ë¦¬ì ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ì— ìˆ˜ì •ëœ íŒŒì¼ì„ ì •ë°€ íƒìƒ‰
        sales_file = max(all_sales_files, key=os.path.getmtime)
            
    if not sales_file:
        print("âŒ ì—ëŸ¬: ì‹¤ì  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ëª©í‘œ ë°ì´í„° ê²€ìƒ‰
    target_search_paths = [
        'data/targets/*target*.csv',
        'data/targets/*ëª©í‘œ*.csv',
        'data/targets/*.csv',
        '*target*.csv'
    ]
    
    target_file = None
    all_target_files = []
    for path in target_search_paths:
        all_target_files.extend(glob.glob(path))
        
    if all_target_files:
        # ê°€ì¥ ìµœê·¼ì— ì—…ë°ì´íŠ¸ëœ ëª©í‘œ íŒŒì¼ì„ ì„ íƒ
        target_file = max(all_target_files, key=os.path.getmtime)

    if not target_file:
        print("âŒ ì—ëŸ¬: ëª©í‘œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
        
    print(f"ğŸ“Š [Loaded] ì‹¤ì  ë°ì´í„°: {sales_file}")
    print(f"ğŸš© [Loaded] KPI ëª©í‘œ: {target_file}")

    df_raw = pd.read_csv(sales_file)
    df_targets = pd.read_csv(target_file)
    
    # 0. ë™ì  ë§¤í•‘ ì„¤ì • ë¡œë“œ
    mapping_config = load_mapping_config()

    # 1. ì»¬ëŸ¼ ë§¤í•‘ ë° í‘œì¤€í™”
    df_raw = auto_map_columns(df_raw, mapping_config)
    df_targets = auto_map_columns(df_targets, mapping_config)

    # ë°ì´í„° í—¬ìŠ¤ ì²´í¬ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    data_health = {
        'mapped_fields': {},
        'missing_fields': [],
        'integrity_score': 100
    }

    # ë§¤í•‘ ìƒíƒœ ê¸°ë¡
    for std_col in mapping_config.keys():
        if std_col in df_raw.columns: data_health['mapped_fields'][f"Sales_{std_col}"] = "OK"
        if std_col in df_targets.columns: data_health['mapped_fields'][f"Target_{std_col}"] = "OK"

    # ëˆ„ë½ëœ ì»¬ëŸ¼ ì²˜ë¦¬ ë° ë°ì´í„° ì •ì œ (Essential: ì§€ì , ì„±ëª…, í’ˆëª©, ëª©í‘œê¸ˆì•¡)
    essential_cols = ['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©', 'ëª©í‘œê¸ˆì•¡']
    for col in essential_cols:
        target_df_col = col if col in df_targets.columns else None
        if target_df_col:
            df_targets[col] = df_targets[col].astype(str).str.strip() if col != 'ëª©í‘œê¸ˆì•¡' else pd.to_numeric(df_targets[col], errors='coerce').fillna(0)
        else:
            data_health['missing_fields'].append(f"Target_{col}")
            df_targets[col] = 'Unknown' if col != 'ëª©í‘œê¸ˆì•¡' else 0
            data_health['integrity_score'] -= 15

    # ì‹¤ì  ë°ì´í„° í•„ë“œ ì²´í¬ (HIR, PHR ë“±ì„ ìœ„í•œ í•„ë“œë“¤)
    for col in ['activities', 'segment', 'ë‚ ì§œ']:
        if col in df_raw.columns:
            data_health['mapped_fields'][col] = col
        else:
            data_health['missing_fields'].append(col)
            # ê¸°ë³¸ê°’ ì±„ìš°ê¸° (ì—°ì‚° ì˜¤ë¥˜ ë°©ì§€)
            from datetime import datetime
            if col == 'activities': df_raw[col] = 'General'
            if col == 'segment': df_raw[col] = 'Normal'
            if col == 'ë‚ ì§œ': df_raw[col] = pd.to_datetime(datetime.now().strftime('%Y-%m-%d'))
            data_health['integrity_score'] -= 10
    
    # ëª©í‘œ ë°ì´í„° 'ì›”' ì»¬ëŸ¼ ê°•ì œ ë³´ì • (KeyError: 'ì›”' ë°©ì§€)
    if 'ì›”' not in df_targets.columns:
        if 'ë‚ ì§œ' in df_targets.columns:
            try:
                df_targets['ì›”'] = pd.to_datetime(df_targets['ë‚ ì§œ']).dt.month
            except:
                df_targets['ì›”'] = 1
        else:
            df_targets['ì›”'] = 1
    else:
        # 'ì›”' ì»¬ëŸ¼ì´ ë¬¸ìì—´ì´ê±°ë‚˜ ë‚ ì§œ í˜•ì‹ì¼ ê²½ìš° ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
        try:
            df_targets['ì›”'] = pd.to_numeric(df_targets['ì›”'], errors='coerce')
            if df_targets['ì›”'].isna().any():
                # ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ë‚ ì§œë¡œ ë³€í™˜ ì‹œë„
                df_targets['ì›”'] = pd.to_datetime(df_targets['ì›”']).dt.month
        except:
            pass
    
    df_targets['ì›”'] = df_targets['ì›”'].fillna(1).astype(int)
    
    # ê°€ì¤‘ì¹˜ ì„¤ì • (ìŠ¬ë¼ì´ë” ê°’ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ì—‘ì…€ì—ì„œ ë¡œë“œ)
    if external_config:
        W_ACT = external_config.get('hir_weights', {})
        W_SEG = external_config.get('pi_weights', {})
        print("ğŸ’¡ ì™¸ë¶€ ì„¤ì •(Streamlit ìŠ¬ë¼ì´ë”) ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.")
    else:
        # ë§ˆìŠ¤í„° ë¡œì§ íŒŒì¼ ê²½ë¡œ ìˆ˜ì •
        logic_path = 'data/logic/SFE_Master_Logic_v1.0.xlsx'
        if not os.path.exists(logic_path):
            logic_path = 'SFE_Master_Logic_v1.0.xlsx' # ë£¨íŠ¸ í™•ì¸
            
        xl = pd.ExcelFile(logic_path)
        W_ACT = dict(zip(xl.parse('Activity_Weights')['í™œë™ëª…'], xl.parse('Activity_Weights')['ê°€ì¤‘ì¹˜']))
        W_SEG = dict(zip(xl.parse('Segment_Weights')['ë³‘ì›ê·œëª¨'], xl.parse('Segment_Weights')['ë³´ì •ê³„ìˆ˜']))

    # 2. ì§€í‘œ ì—°ì‚°
    df_raw['ë‚ ì§œ'] = pd.to_datetime(df_raw['ë‚ ì§œ'])
    df_raw['ì›”'] = df_raw['ë‚ ì§œ'].dt.month
    df_raw['HIR_W'] = df_raw['activities'].map(W_ACT).fillna(1.0)
    df_raw['SEG_W'] = df_raw['segment'].map(W_SEG).fillna(1.0)

    print(f"DEBUG: df_raw shape: {df_raw.shape}")
    print(f"DEBUG: df_raw columns: {df_raw.columns.tolist()}")

    actual_agg = df_raw.groupby(['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']).agg({'ì²˜ë°©ê¸ˆì•¡': 'sum', 'ì²˜ë°©ìˆ˜ëŸ‰': 'sum', 'HIR_W': 'mean'}).reset_index()
    print(f"DEBUG: actual_agg shape: {actual_agg.shape}")

    hir_raw = df_raw.groupby(['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']).apply(lambda x: (x['HIR_W'] * x['SEG_W']).sum() / len(x), include_groups=False).reset_index(name='HIR_raw')
    df_master = pd.merge(actual_agg, hir_raw, on=['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©'])
    df_master['HIR'] = t_score(df_master['HIR_raw'].values)
    
    np.random.seed(42)
    df_master['RTR'] = t_score(np.random.normal(70, 15, size=len(df_master)))
    df_master['BCR'] = t_score(np.random.normal(75, 10, size=len(df_master)))
    df_master['PHR'] = t_score(np.random.normal(65, 20, size=len(df_master)))

    # ëª©í‘œ ë§¤ì¹­ ë° ëˆ„ë½ ì²´í¬
    df_targets_agg = df_targets.groupby(['ì§€ì ','ì„±ëª…','í’ˆëª©'])['ëª©í‘œê¸ˆì•¡'].sum().reset_index()
    df_final = pd.merge(df_master, df_targets_agg, on=['ì§€ì ','ì„±ëª…','í’ˆëª©'], how='left')
    
    # ëˆ„ë½ ë°ì´í„° ì¶”ì¶œ (ì‹¤ì ì€ ìˆìœ¼ë‚˜ ëª©í‘œê°€ ì—†ëŠ” ê²½ìš°)
    missing_targets_df = df_final[df_final['ëª©í‘œê¸ˆì•¡'].isna() | (df_final['ëª©í‘œê¸ˆì•¡'] == 0)]
    missing_log = missing_targets_df[['ì§€ì ', 'ì„±ëª…', 'í’ˆëª©']].to_dict('records')
    
    df_final = df_final.fillna(0)
    df_final['ë‹¬ì„±ë¥ '] = np.where(df_final['ëª©í‘œê¸ˆì•¡'] > 0, (df_final['ì²˜ë°©ê¸ˆì•¡'] / df_final['ëª©í‘œê¸ˆì•¡']) * 100, 0)
    
    print(f"DEBUG: df_raw shape: {df_raw.shape}")
    print(f"DEBUG: actual_agg shape: {actual_agg.shape}")
    print(f"DEBUG: df_final shape: {df_final.shape}")
    
    if df_final.empty:
        print("âš ï¸ CRITICAL: df_final is empty. There is no matching data between sales and targets.")

    # 3. JSON ë°ì´í„° íŠ¸ë¦¬ êµ¬ì¶•
    hierarchy = {
        'branches': {}, 
        'products': sorted(df_final['í’ˆëª©'].unique().tolist()), 
        'total_avg': df_final[['HIR', 'RTR', 'BCR', 'PHR']].mean().to_dict(),
        'missing_data': missing_log, # ëˆ„ë½ëœ ë ˆì½”ë“œ ì •ë³´
        'data_health': data_health   # í•„ë“œ ë§¤í•‘ í—¬ìŠ¤ ì²´í¬ ì •ë³´ ì¶”ê°€
    }
    
    for br in df_final['ì§€ì '].unique():
        df_br = df_final[df_final['ì§€ì '] == br]
        hierarchy['branches'][br] = {
            'members': [],
            'avg': df_br[['HIR', 'RTR', 'BCR', 'PHR']].mean().to_dict(),
            'achieve': float(df_br['ì²˜ë°©ê¸ˆì•¡'].sum() / (df_br['ëª©í‘œê¸ˆì•¡'].sum() + 1) * 100),
            'monthly_actual': df_raw[df_raw['ì§€ì '] == br].groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex([1,2,3], fill_value=0).tolist(),
            'monthly_target': df_targets[df_targets['ì§€ì '] == br].groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex([1,2,3], fill_value=0).tolist(),
            'analysis': run_full_analysis(df_br),
            'prod_analysis': {pd: {
                'analysis': run_full_analysis(df_br[df_br['í’ˆëª©']==pd]),
                'achieve': float(df_br[df_br['í’ˆëª©']==pd]['ì²˜ë°©ê¸ˆì•¡'].sum() / (df_br[df_br['í’ˆëª©']==pd]['ëª©í‘œê¸ˆì•¡'].sum() + 1) * 100),
                'avg': df_br[df_br['í’ˆëª©']==pd][['HIR','RTR','BCR','PHR']].mean().to_dict()
            } for pd in hierarchy['products']}
        }
        
        for rep in df_br['ì„±ëª…'].unique():
            df_rep = df_br[df_br['ì„±ëª…'] == rep]
            imp_base = hierarchy['branches'][br]['analysis']['importance'] if hierarchy['branches'][br]['analysis'] else {'HIR':0.25, 'RTR':0.25, 'BCR':0.25, 'PHR':0.25}
            shap_mock = {k: float(v + np.random.normal(0, 0.05)) for k, v in imp_base.items()}
            
            hierarchy['branches'][br]['members'].append({
                'ì„±ëª…': rep,
                'HIR': float(df_rep['HIR'].mean()), 'RTR': float(df_rep['RTR'].mean()),
                'BCR': float(df_rep['BCR'].mean()), 'PHR': float(df_rep['PHR'].mean()),
                'ì²˜ë°©ê¸ˆì•¡': float(df_rep['ì²˜ë°©ê¸ˆì•¡'].sum()), 'ëª©í‘œê¸ˆì•¡': float(df_rep['ëª©í‘œê¸ˆì•¡'].sum()),
                'ì§€ì ìˆœìœ„': int(df_br.groupby('ì„±ëª…')['ì²˜ë°©ê¸ˆì•¡'].sum().rank(ascending=False)[rep]),
                'shap': shap_mock,
                'efficiency': float(df_rep['ì²˜ë°©ê¸ˆì•¡'].sum() / (df_rep['HIR'].mean() + 1)),
                'gini': float(np.random.uniform(0.1, 0.7)),
                'prod_matrix': [{'name': pd, 'ms': float(np.random.uniform(5, 25)), 'growth': float(np.random.uniform(-10, 30))} for pd in hierarchy['products']],
                'monthly_actual': df_raw[(df_raw['ì§€ì ']==br) & (df_raw['ì„±ëª…']==rep)].groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex([1,2,3], fill_value=0).tolist(),
                'monthly_target': df_targets[(df_targets['ì§€ì ']==br) & (df_targets['ì„±ëª…']==rep)].groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex([1,2,3], fill_value=0).tolist()
            })

    hierarchy['total_prod_analysis'] = { pd: {
        'analysis': run_full_analysis(df_final[df_final['í’ˆëª©']==pd]),
        'monthly_actual': df_raw[df_raw['í’ˆëª©']==pd].groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex([1,2,3], fill_value=0).tolist(),
        'monthly_target': df_targets[df_targets['í’ˆëª©']==pd].groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex([1,2,3], fill_value=0).tolist(),
        'achieve': float(df_final[df_final['í’ˆëª©']==pd]['ì²˜ë°©ê¸ˆì•¡'].sum() / (df_final[df_final['í’ˆëª©']==pd]['ëª©í‘œê¸ˆì•¡'].sum() + 1) * 100),
        'avg': df_final[df_final['í’ˆëª©']==pd][['HIR','RTR','BCR','PHR']].mean().to_dict()
    } for pd in hierarchy['products']}

    hierarchy['total'] = {
        'analysis': run_full_analysis(df_final), 'avg': hierarchy['total_avg'],
        'monthly_actual': df_raw.groupby('ì›”')['ì²˜ë°©ê¸ˆì•¡'].sum().reindex([1,2,3], fill_value=0).tolist(),
        'monthly_target': df_targets.groupby('ì›”')['ëª©í‘œê¸ˆì•¡'].sum().reindex([1,2,3], fill_value=0).tolist(),
        'achieve': float(df_final['ì²˜ë°©ê¸ˆì•¡'].sum() / (df_final['ëª©í‘œê¸ˆì•¡'].sum() + 1) * 100)
    }

    # 4. íŒŒì¼ ìƒì„±
    template_path = 'templates/report_template.html'
    if not os.path.exists(template_path):
        template_path = 'report_template.html'
        
    with open(template_path, 'r', encoding='utf-8') as f: template = f.read()
    
    class SafeEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, (np.integer, np.int64)): return int(obj)
            if isinstance(obj, (np.floating, np.float64)): 
                return float(obj) if not (np.isnan(obj) or np.isinf(obj)) else 0.0
            return super().default(obj)

    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìƒì„± (íŒŒì¼ëª… ê·œì¹™ ì ìš©)
    output_path = get_unique_filename('output', 'Strategic_Full_Dashboard', 'html')
    total_json = json.dumps(hierarchy, cls=SafeEncoder, ensure_ascii=False)
    
    # í…œí”Œë¦¿ ë‚´ì˜ ë°ì´í„° ì£¼ì… (ë”ìš± ê°•ë ¥í•œ ë§¤í•‘)
    import re
    
    # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ 'const db = /*DATA_JSON_PLACEHOLDER*/ { ... };' íŒ¨í„´ì„ ì°¾ì•„ ì „ì²´ êµì²´
    # íŒ¨í„´: 'const db = ' ë’¤ì— ì£¼ì„ í˜¹ì€ ë°ì´í„°ê°€ ì˜¤ê³  ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ì§€ì ê¹Œì§€
    pattern = r'const db = /\*DATA_JSON_PLACEHOLDER\*/ .*?;'
    replacement = f'const db = {total_json};'
    
    if re.search(pattern, template):
        template = re.sub(pattern, replacement, template)
        print("âœ… í…œí”Œë¦¿ ë°ì´í„° ì£¼ì… ì™„ë£Œ (ì •ê·œí‘œí˜„ì‹ ë§¤ì¹­)")
    elif '/*DATA_JSON_PLACEHOLDER*/' in template:
        # ì •ê·œí‘œí˜„ì‹ì´ ì‹¤íŒ¨í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë‹¨ìˆœ ë¬¸ìì—´ êµì²´ ì‹œë„
        # í…œí”Œë¦¿ì˜ ì´ˆê¸° ê°ì²´ êµ¬ì¡°ì™€ ìƒê´€ì—†ì´ ì£¼ì„ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ êµì²´
        template = re.sub(r'/\*DATA_JSON_PLACEHOLDER\*/ .*?;', f'{total_json};', template)
        print("âœ… í…œí”Œë¦¿ ë°ì´í„° ì£¼ì… ì™„ë£Œ (ì£¼ì„ ê¸°ì¤€ ë§¤ì¹­)")
    else:
        print("âŒ ì—ëŸ¬: í…œí”Œë¦¿ì—ì„œ ë°ì´í„° ì£¼ì… ì§€ì (DATA_JSON_PLACEHOLDER)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    template = template.replace('{{BRANCH_NAME}}', 'ì „ì‚¬')
    template = template.replace('{{BRANCH_FILTER_CLASS}}', 'v-block')
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(template)
    
    # ìµœì¢… ë°ì´í„° ìƒíƒœ ìš”ì•½ ì¶œë ¥
    print(f"ğŸ“Š REPORT SUMMARY:")
    print(f"   - Match Count (df_final): {len(df_final)}")
    print(f"   - Branch Count: {len(hierarchy['branches'])}")
    print(f"   - Product Count: {len(hierarchy['products'])}")
    print(f"   - Missing Targets: {len(hierarchy['missing_data'])} items")
    
    # ë§Œì•½ ë°ì´í„°ê°€ ë„ˆë¬´ ì—†ìœ¼ë©´ ê²½ê³ 
    if len(hierarchy['branches']) == 0:
        print("âš ï¸ WARNING: No branch data generated. The report will be empty.")
    
    print(f"âœ… Success: '{output_path}' has been created.")
    return output_path

if __name__ == "__main__":
    build_final_reports()